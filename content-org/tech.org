#+author: yuan.tops@gmail.com
#+hugo_base_dir: ../
#+HUGO_SECTION: tech
# Categories
#+filetags: @Tech
#+hugo_auto_set_lastmod: t

* All about this blog                                                           :gh_pages:hugo:
** DONE How to blog with ox-hugo in Emacs                                       :Emacs:
   CLOSED: [2019-07-25 Thu 14:47]
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_DATE: "2019-07-24T22:29:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2019-07-24T22:29:52Z"
:EXPORT_FILE_NAME: blogging-with-ox-hugo
:EXPORT_DESCRIPTION: 发现一个用Emacs写blog的新工具: ox-hugo。它与org-mode融合得非常自然，更好用。本文记录如何在Emacs中配置与使用ox-hugo。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "ox-hugo" )
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2019-07-25 Thu 14:47]
- State "TODO"       from "DONE"       [2019-07-25 Thu 10:05]
:END:

我的[[https://blog.yuantops.com/tech/emacs-orgmode-hugo-with-oxpandoc/][一篇老博客]]，介绍了用Emacs和Org-mode写博客的工作流。总的来说，勉强符合预期，流程稍显磕绊。今天偶然发现，用Org-mode写博客有了正规军： ~ox-hugo~ 。简单试用之后,发现 ~ox-hugo~ 表现很流畅，几乎就是对之前流程的升级，于是毫不犹豫选 ~ox-hugo~ 。

*** 安装
~ox-hugo~ 是一个工作在Org-mode的Emacs包，安装过程可以说平平无奇：
1. ~M x~ ~package-refresh-contents~ :: 更新安装源
2. 添加Emacs配置。修改细节与Emacs配置风格相关，我使用[[https://github.com/purcell/emacs.d][Purcell维护的.emacs.d配置]]，仅供参考。
   - 在 =.emacs.d/lisp/= 目录下，新增配置文件 ~init-ox-hugo.el~
     #+BEGIN_SRC emacs-lisp -n
     (require-package 'ox-hugo)

     (with-eval-after-load 'ox (require 'ox-hugo))

     (provide 'init-ox-hugo)

     #+END_SRC

   - 在入口配置文件 ~.emacs.d/init.el~ 中，引用新增的配置文件
     #+BEGIN_SRC lisp

     (require 'init-ox-hugo)

     #+END_SRC

*** 写博客旧流程
现在，org文件可以直接导出为Hugo支持的Markdown格式了。这一点 ~ox-pandoc~ 也能做到，但 ~ox-hugo~ 还能做得更多。

回顾之前的工作流:
1. 新建xx.org文件
2. 输入二级标题，接着插入 ~front matter~ 。 ~front matter~ 内容大致如下：
   #+BEGIN_SRC yaml
   +++
   title = ""
   date = ""
   Categories = ["Tech"]
   Tags = ["Emacs"]
   Description = ""
   keywords = [""]
   +++
   #+END_SRC
3. 写博客正文
4. 按下 ~C-c C-e~ , 将二级标题对应的subtree导出为Markdown格式文件
5. 将Markdown格式文件保存到contents/tech/目录。

~front matter~ 是hugo渲染文件需要的必要元信息, 例如文章标签、分类、标题等。ox-hugo改进之一，是使用org-mode语法放置元信息，不再需要手动设置 ~front matter~ 。

Org-mode支持为headline设置 tags。 ~ox-hugo~ 沿用这点，org-mode的tag就是文章标签, 以@开头的tag就是文章分类。其他例如作者、日期、文件名的元信息，通过 ~:PROPERTIES:~ 设置。在导出Markdown格式时，ox-hugo会自动提取这些数据。因为ox-hugo可以一次导出所有subtree，因此 ~ox-hugo~ 官方推荐做法是，所有文章放到一个org文件，[[https://ox-hugo.scripter.co/doc/screenshots/#screenshot-one-post-per-subtree][每个Subtree对应一篇文章]]。

对于习惯一个org文件对应一篇文章的作者而言，ox-hugo也予以尊重，[[https://ox-hugo.scripter.co/doc/org-meta-data-to-hugo-front-matter/][在文件头添加对应配置项即可]]。

*** 我的实践
对本博客而言，考虑到1)文章固定只有3个分类，2)现在已有很多存量md格式的文章，我采取一种折中做法: 一个分类对应一个org文件，org文件里每个subtree对应一篇文章。这样兼顾现状，而且新文章顺利迁移到新做法。就每个org文件来说，元信息分为两大类：
1. 对整个文件都生效的配置，放到文件开头；
   #+BEGIN_SRC yaml
   #+author: yuan.tops@gmail.com
   #+hugo_base_dir: ../
   #+HUGO_SECTION: tech
   # Categories
   #+filetags: @tech
   #+hugo_auto_set_lastmod: t
   #+END_SRC
2. 对单篇文章生效的配置，放到subtree的标题下面。
   #+BEGIN_SRC props
   :PROPERTIES:
   :EXPORT_HUGO_CATEGORIES: Tech
   :EXPORT_DATE: "2019-07-24T22:29:52+08:00"
   :EXPORT_HUGO_PUBLISHDATE: "2019-07-24T22:29:52Z"
   :EXPORT_FILE_NAME: blogging-with-ox-hugo
   :EXPORT_DESCRIPTION: 发现一个用Emacs写blog的新工具: ox-hugo。它与org-mode融合得非常自然，更好用。本文记录如何在Emacs中配置与使用ox-hugo。
   :END:
   #+END_SRC

以本文为例，配置如图:
[[file:screenshot-org-subtree.png]]

*** 总结
Emacs学习曲线盘旋上升, Org-mode也是如此。不断折腾，乐在其中！


** 一次顿悟                                                     :Emacs:
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_DATE: "2021-09-01T17:33:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2021-09-01T17:33:52+08:00"
:EXPORT_FILE_NAME: some_eureka_moment
:EXPORT_DESCRIPTION: 对于Emacs的一次思考，以及收获……
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "ox-hugo" )
:END:

最近用 ~Emacs~ ~ox-hugo~ 写了几篇文章，体验比较糟，表现为总是无理由卡顿，特别是在 org 页面按下 `Ctrl-C, Ctrl-E`导出markdown时直接卡死，好几次只能强行杀掉Emacs 进程。

总所周知，Emacs 是神的编辑器，肯定不会出问题；我不爽，必定是我不会用了。于是，我鼓足勇气，想看看是哪里出了问题。

经过辛勤搜索，各种修改配置，纹丝不动。焦躁之余，不禁回忆起往事……

原本我是坚定vim党，自得其乐。

后来，受网络meme影响，终于买了一本讲Emacs的电子书，开始像八爪章鱼一样学按键，键位那么多，组合那么多。。。草草翻完一遍，只剩下 "这也行！" 的感叹号。试了 ~Dired~, ~Magit~, 自觉太复杂，难以记忆。放弃之。

又过了一段时间，心又痒痒了……这次吃了 ~org~ 的安利，GTD 好厉害！！大概实践了两个星期，只记得按 ~Tab~ 键可以切换标题等级。

下一次入坑，是受到 ~ox-hugo~ 的蛊惑: 确实如行云流水，非常满意。

想到这里，我顿悟了，我只是为了 *org-mode 和 ox-hugo 啊* ！ Emacs 再怎么厉害，别人用得再怎么出神入化，用它写代码(特别写Java!!)、看小说、玩游戏、看视频…… 和我有什么关系呢? 弱水三千，只需取一瓢。

于是，爽爽快快删掉攒了多年的配置: 

     #+BEGIN_SRC shell
       rm -fr .emacs.d
     #+END_SRC

热情投向 ~spacemacs~ 怀抱:
     #+BEGIN_SRC shell
       git clone -b develop https://github.com/syl20bnr/spacemacs .emacs.d
     #+END_SRC

启动emacs。因为是spacemacs第一次加载配置文件，会有一些询问交互。一路选择最小化配置。

最后修改 ~.spacemacs~, 加上 ~ox-hugo~ 支持: 

     #+BEGIN_SRC emacs-lisp -n
       (setq-default dotspacemacs-configuration-layers
                     '((org :variables
                         org-enable-hugo-support t)))
     #+END_SRC

重新加载配置，完完全全就好了！

恭喜我自己 :)

** DONE 浏览器会处理URL里的相对路径
   CLOSED: [2019-11-21 Thu 15:53]
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: how_web_broswer_handles_url_relative_path
:EXPORT_DESCRIPTION: 如果要访问的url包含相对路径，浏览器会尝试解析相对路径，再访问解析得到的地址。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "http" )
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2019-11-21 Thu 15:53]
:END:

新系统上线前，安全部门扫描出一个高危漏洞：文件任意下载漏洞。渗透测试人员在URL里加上相对路径，不断发起HTTP请求，居然成功下载到Linux系统密码文件。修复漏洞挺简单，限制HTTP服务访问文件系统权限，不允许超出指定目录，几行代码搞定。

修复之后准备进行验证，第一步当然是复现漏洞。没想到，这一步就挺曲折。

*** 消失的点号
打开chrome，在地址栏输入带了相对路径( *..* )的URL。URL指向一个文件，理论上，会触发文件下载。结果是：地址栏URL路径里点号全不见了，变成了一个正常地址。多试几遍，把双点号换成单点号，仍然如此。用 wiresharks 抓包看HTTP报文，请求头 /path/ 没有点号。这说明，浏览器做了手脚。

1. Google搜之，找到一份解析URI的RFC3968标准，专门有一章论述解析点号：[[https://tools.ietf.org/html/rfc3986#section-5.2.4][Remove Dot Segments]]。经过解析，点号和双点号会消失，这个过程被称为 *remove_dot_segments* 。(RFC3968给出了这个过程的伪代码。)

2. Google官方在一篇文章里，将Chrome解析URL的过程称为 `Canonicalization` ([[https://chromium.googlesource.com/chromium/src/+/master/docs/security/url_display_guidelines/url_display_guidelines.md#display-urls-in-canonical-form][display-urls-in-canonical-form]]) 。经过解析，Chrome地址栏的点号变成实际值。

结合两篇文档，原理清楚了：浏览器遵循RFC3968规范处理URL相对路径，所以点号和双点号都被干掉了。

*** 改用Burp Suite重现问题
不能用浏览器复现问题，改尝试 *curl* 命令。结果，curl也不能复现。好在可以借助 *Burp Suite* 工具。

Burp Suite是一款攻击web服务的集成工具，一般黑客用它来渗透网络。我们牛刀小用，用来拦截、修改HTTP请求报文。过程不在此赘述。总之，用它绕过了相对路径解析、重现了漏洞。

*** 修复漏洞
略。


* Raspeberry Tutorials                                                          :树莓派:
** TODO 用树莓派和Calibre搭建电子书服务器

* Road to linux expert                                                          :Linux:
** Bash Guideline Notes
:PROPERTIES:
:EXPORT_DATE: "2019-07-25T22:29:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2019-07-25T22:29:52Z"
:EXPORT_FILE_NAME: bash-guideline-study-notes
:EXPORT_DESCRIPTION: 《Bash Guideline》摘抄与笔记
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "bash" )
:END:
*** 关于重定向顺序
#+BEGIN_QUOTE

Note that the order of redirections is signi cant. For example, the command \\
\\
ls > dirlist 2>&1 \\
directs both standard output ( file descriptor 1) and standard error ( le descriptor 2) to the file dirlist, while the command ls 2>&1 > dirlist directs only the standard output to file dirlist, because the standard error was made a copy of the standard output before the standard output was redirected to dirlist. \\
#+END_QUOTE

*** 将Stdout 和 Stderr 重定向到 文件
#+BEGIN_QUOTE
This construct allows both the standard output ( file descriptor 1) and the standard error output ( file descriptor 2) to be redirected to the file whose name is the expansion of word. \\
\\
There are two formats for redirecting standard output and standard error:\\
\\
&>word and \\
\\
>&word
\\
Of the two forms, the first is preferred. This is semantically equivalent to\\
>word 2>&1\\
#+END_QUOTE

*** Here Document

#+BEGIN_SRC
Here Documents
This type of redirection instructs the shell to read input from the current source until a line containing only word (with no trailing blanks) is seen.

All of the lines read up to that point are then used as the standard input for a command.

The format of here-documents is:

          <<[-]word
                  here-document
          delimiter
No parameter expansion, command substitution, arithmetic expansion, or pathname expansion is performed on word. If any characters in word are quoted, the delimiter is the result of quote removal on word, and the lines in the here-document are not expanded. If word is unquoted, all lines of the here-document are subjected to parameter expansion, command substitution, and arithmetic expansion. In the latter case, the character sequence \<newline> is ignored, and \ must be used to quote the characters \, $, and `.

If the redirection operator is <<-, then all leading tab characters are stripped from input lines and the line containing delimiter. This allows here-documents within shell scripts to be indented in a natural fashion.

$ cat <<EOF > print.sh
#!/bin/bash
echo \$PWD
echo $PWD
EOF
#+END_SRC

** Understanding XOR
:PROPERTIES:
:EXPORT_DATE: "2019-07-25T15:29:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2019-07-25T22:29:52Z"
:EXPORT_FILE_NAME: understanding-xor
:EXPORT_DESCRIPTION: 理解xor
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "xor" )
:END:

#+BEGIN_QUOTE
We can interpret the action of XOR in a number of different ways, and this helps to shed light on its properties. The most obvious way to interpret it is as its name suggests, ‘exclusive OR’: A ⊕ B is true if and only if precisely one of A and B is true. Another way to think of it is as identifying difference in a pair of bytes: A ⊕ B = ‘the bits where they differ’. This interpretation makes it obvious that A ⊕ A = 0 (byte A does not differ from itself in any bit) and A ⊕ 0 = A (byte A differs from 0 precisely in the bit positions that equal 1) and is also useful when thinking about toggling and encryption later on. \\
\\
The last, and most powerful, interpretation of XOR is in terms of parity, i.e. whether something is odd or even. For any n bits, A1 ⊕ A2 ⊕ … ⊕ An = 1 if and only if the number of 1s is odd. This can be proved quite easily by induction and use of associativity. It is the crucial observation that leads to many of the properties that follow, including error detection, data protection and adding. \\
\\
 Essentially the combined value x ^ y ‘remembers’ both states, and one state is the key to getting at the other.
#+END_QUOTE

** TCP TIME_WAIT 连接太多
:PROPERTIES:
:EXPORT_DATE: "2021-08-24T15:29:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2021-08-24T22:29:52Z"
:EXPORT_FILE_NAME: linux_tcp_time_wait_tuning
:EXPORT_DESCRIPTION: 服务器发起短连接过多，可能出现端口耗尽情况。通过调整 ipv4 参数解决。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "tcp" "time_wait" )
:END:

压测一个服务，性能卡住了上不去。错误信息提示是没有可分配端口。搜索发现别人也遇到过类似问题([[https://www.cnblogs.com/softidea/p/6062147.html][linux 大量的TIME_WAIT解决办法]])。    

把解决配置摘录如下：

配置 tcp 连接参数 vim /etc/sysctl.conf 
编辑文件，加入以下内容：
#+BEGIN_SRC
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
#+END_SRC

另外，也要关注系统本身对资源限制: 
配置 /etc/security/limits.conf，把值加大:
#+BEGIN_SRC 
        *       soft    nofile  65535
        *       hard    nofile  65535
        *       soft    nproc  65535
        *       hard    nproc  65535
#+END_SRC

***  net.ipv4.tcp_fin_timeout 做了啥?
Stackoverflow 网友[[https://stackoverflow.com/questions/46066046/unable-to-reduce-time-wait][如是说]]:
#+BEGIN_SRC
Your link is urban myth. The actual function of net.ipv4.tcp_fin_timeout is as follows:

This specifies how many seconds to wait for a final FIN packet before the socket is forcibly closed. This is strictly a violation of the TCP specification, but required to prevent denial-of-service attacks. In Linux 2.2, the default value was 180.

This doesn't have anything to do with TIME_WAIT. It establishes a timeout for a socket in FIN_WAIT_1, after which the connection is reset (which bypasses TIME_WAIT altogether). This is a DOS measure, as stated, and should never arise in a correctly written client-server application. You don't want to set it so low that ordinary connections are reset: you will lose data. You don't want to fiddle with it at all, actually.
#+END_SRC

是时候破除迷思了！这个参数和 *TIME_WAIT* 没有直接关系。根据TCP/IP状态机，主动发起关闭的一方，将进入 *FIN_WAIT_1* 状态，等待接收 *FIN* 报文。 *net.ipv4.tcp_fin_timeout* 规定在 *FIN_WAIT_1* 状态的停留时间。时间一到，跳过 *TIME_WAIT* 状态，连接被强行关闭。

** nginx 反向代理不开启http1.1时的行为探究
:PROPERTIES:
:EXPORT_DATE: "2021-08-24T14:29:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2021-08-24T14:33:52Z"
:EXPORT_FILE_NAME: nginx_proxy_pass_without_enable_http1.1
:EXPORT_DESCRIPTION: nginx 做反向代理服务器，与upstream之间默认使用http1.0协议。借助tcpdump和wireshark,来看看开启http1.1前后的区别。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "nginx" )
:END:

最近给一个tomcat服务加上nginx代理，陆续遇到一些问题（坑）。    

第一个坑，一个接口直接访问正常，经nginx代理后报104错误(104: Connection reset by peer)。奇葩之处在于，只有特定的接口出现这种错。

*** 先说解决方案：配置反向代理长链接

很容易搜到104错误的解决方案.在nginx配置中，加上下面两句:

#+BEGIN_SRC
        proxy_http_version 1.1;    
        proxy_set_header Connection "";
#+END_SRC

加上之后，执行命令 *nginx -s reload* 生效。

翻看nginx[[http://nginx.org/en/docs/http/ngx_http_proxy_module.html#keepalive][官方文档]]，这么说: 
#+BEGIN_SRC

For HTTP, the proxy_http_version directive should be set to “1.1” and the “Connection” header field should be cleared:
upstream http_backend {
    server 127.0.0.1:8080;

    keepalive 16;
}

server {
    ...

    location /http/ {
        proxy_pass http://http_backend;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        ...
    }
}
#+END_SRC

到这里，问题已经解决。我们再进一步，看看配置前后的区别。

*** tcpdump 抓包
在修改前后，各自抓包。
#+BEGIN_QUOTE sh
# ens是网卡接口名,80是nginx端口，8082是上游端口。抓包数据保存到 tcpdump.cap
tcpdump -iensxxx -vvvs0 -l -A 'tcp port 80 or tcp port 8082' -w tcpdump.cap
#+END_QUOTE

*** wireshark分析
将*.cap文件用wireshark 打开. 
1. 在filter里输入 *http.request && http contains "HTTP/1.0"* ，过滤出nginx到上游服务的请求.
2. 随意选中一条记录。右键，"追踪流..." -> "TCP流"。这样，筛选出了TCP会话的所有包记录。
3. 右键, "复制" -> "摘要为文本"，可以将报文导出为文本。

下面的记录，第一条是异常响应报文，第二条是正常响应报文。
#+BEGIN_SRC
--- not ok tcp.stream eq 75
HTTP/1.1 200 OK
Server: Apache-Coyote/1.1
Content-Type: application/json;charset=UTF-8
Date: Mon, 23 Aug 2021 09:41:57 GMT
Connection: close

[{"xxxxxx":"xxxxxxxxxxxxxxx","xxxx":"xxx","xxxxxx":"xxxxxxxxxxxxx","xxx":xx}]


--- ok tcp.stream eq 104
HTTP/1.1 200 OK
Server: Apache-Coyote/1.1
Content-Type: application/json;charset=UTF-8
Transfer-Encoding: chunked
Date: Mon, 23 Aug 2021 09:45:33 GMT

4d
[{"xxxxxx":"xxxxxxxxxxxxxxx","xxxx":"xxx","xxxxxx":"xxxxxxxxxxxxx","xxx":xx}] 
#+END_SRC


可以观察到:
1. 头部不一致。
   - 异常报文多了 "Connection: close" ，含义是数据返回后将关闭连接。
   - 正常报文多了 "Transfer-Encoding: chunked"，含义是返回的数据采取“分块编码”, 也就是分多块返回。
2. body不一致。
   - 正常报文在body之前，声明了数据块的大小。
3. 我们使用HTTP1.0发起请求，但返回报文头部居然写着 HTTP/1.1。这是怎么回事??
   参考 https://stackoverflow.com/questions/19461312/why-tomcat-reply-http-1-1-respose-with-an-http-1-0-request, 返回头的HTTP版本号只是一个声明，告知调用方服务端所支持的最高HTTP版本。
#+BEGIN_SRC

This Connector supports all of the required features of the HTTP/1.1 protocol, as described in RFC 2616, including persistent connections, pipelining, expectations and chunked encoding. If the client (typically a browser) supports only HTTP/1.0, the Connector will gracefully fall back to supporting this protocol as well. No special configuration is required to enable this support. The Connector also supports HTTP/1.0 keep-alive.

RFC 2616 requires that HTTP servers always begin their responses with the highest HTTP version that they claim to support. Therefore, this Connector will always return HTTP/1.1 at the beginning of its responses.
#+END_SRC

*** 为什么有的接口不报错?
在此之前，先思考一个问题：http协议是基于tcp的，tcp本身不处理数据包的边界，那客户端怎么做到恰到好处地读取数据？

无外乎两种方案：1. 在报文中，先声明数据大小，客户端读满为止；2.约定一个边界，客户端看到这个记号再停。

HTTP/1.0中，采用第一种方案，在HTTP头部利用 “Content-Length”声明http body的长度; 

HTTP/1.1协议引入 "Transfer-Encoding: chunked"头。浏览器不需要等到内容字节全部下载完成,只要接收到一个chunked块就可解析页面。这种情况下，每个chunk块之前会声明chunk大小，之后有分隔符。

有了这些思考，再用wireshark来分析HTTP/1.0请求的返回报文。正常接口返回的TCP报文，虽然HTTP头部没有声明Content-Length，但因为数据量大，被拆分为若干个Frame。报错接口返回的报文，数据量很小。

我的推测是：虽然它们都没有显式声明大小或者边界，但是当数据量大到被拆分为多个Frame时，客户端能基于Frame解析出来。


** H2 Database hack —— 批量插入的猥琐实现
:PROPERTIES:
:EXPORT_DATE: "2021-08-27T15:29:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2021-08-27T22:29:52Z"
:EXPORT_FILE_NAME: h2_database_hack_batch_insert
:EXPORT_DESCRIPTION: 通过SQL实现的批量插入都不够快！本文分享一种猥琐实现：把数据直接灌到h2底层数据表。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "H2" "batch" )
:END:

H2 数据库是一款优秀的内存数据库，它具备几个特点：体积小，文档全，功能完善，而且是Java写的。

最近用到它这些优良特性，做内存计算。以内存模式启动了一个H2实例。接下来，要把外部数据导入H2数据库。这就面临一个问题：数据量大（几万+）的情况下，如何保证插入速度？

*** 常规方案
随便一种JDBC 持久层工具, 例如 *JdbcTemplate*, *MyBatis*,都封装了批量接口。怀着封装越少、效率越高的朴素信念，用H2原生JDBC Connection.insert() 方法，循环插入。2.7 万条数据，耗时约 3s。

另外，h2 database 官方有一种做法：把数据先导到 csv 文件，然后加载csv。虽没有实际验证这种方案，但纸上谈兵分析，即使数据加载变快，但增加了两次I/O。效果估计不会特别优秀。

*** 快速方案
同事脑洞大开：内存数据库插入语句，先是SQL解析，再把Java对象写进内存。既然都是Java 对象，能不能跳过SQL这一遭，直接写内存?

不经过JDBC，不经过SQL，这种思路也是不按常规出牌了。但原理非常说得通，而且肯定更快。
 驱动器 E 中的卷没有标签。
 卷的序列号是 DEC2-AD02

 e:\blog\content-org 的目录

2021/08/30  18:31    <DIR>          .
2021/08/30  18:31    <DIR>          ..
2019/07/25  09:12               376 life.org
2021/02/08  09:58            33,210 opinions.org
2019/07/25  14:42            50,696 screenshot-org-subtree.png
2019/07/25  14:45            17,974 tech.html
2021/08/30  18:28            68,217 tech.org
               5 个文件        170,473 字节
               2 个目录 333,343,010,816 可用字节

经过一步步断点调试，找到了关键类: *org.h2.table.Table* 。insert() 语句走到最后，是往table 里添加行(*org.h2.result.Row*)。换言之，只要拿到 table，又按格式构造行，就可以了。

- 获取Table
    按作者原意，应该是不希望使用者直接操作 Table 对象的。但是架不住我们猥琐啊，借助反射机制，什么都拿得到。    
    下面，是一步步抠出 Table 对象的实现。
  
        #+BEGIN_SRC java
         String sql = "select * from " + tableName;
        try (JdbcPreparedStatement ps = (JdbcPreparedStatement) connection.prepareStatement(sql)) {
            CommandContainer commandContainer = (CommandContainer) getFieldByForce(ps, JdbcPreparedStatement.class,
                    "command");
            Session session = (Session) getFieldByForce(ps, JdbcPreparedStatement.class, "session");
            Select command = (Select) getFieldByForce(commandContainer, CommandContainer.class, "prepared");
            Table table = new ArrayList<>(command.getTables()).get(0);

        #+END_SRC

- 构造行
    待插入的数据格式是Map, key是列名，value是值。对应到 *org.h2.result.Row* 的话 ，map每个entry对应一列。当然，涉及一些列名提取与转化，数据类型处理的工作。
    下面是构造行的实现。

        #+BEGIN_SRC java
        Row newRow = table.getTemplateRow();
        Column[] columns = table.getColumns();
        for (Column c : columns) {
            int index = c.getColumnId();
            String columnName = c.getName();
            if (!map.containsKey(columnName)) {
                newRow.setValue(c.getColumnId(), ValueNull.INSTANCE);
            } else {
                Object value = map.get(columnName);
                if (value instanceof String) {
                    newRow.setValue(index, ValueString.get(value.toString()));
                } else if (value instanceof Integer) {
                    newRow.setValue(index, ValueInt.get((Integer) value));
                } else if (value instanceof Timestamp) {
                    newRow.setValue(index, ValueTimestamp.get(TimeZone.getDefault(), (Timestamp) value));
                } else if (value instanceof BigDecimal) {
                    newRow.setValue(index, ValueDecimal.get((BigDecimal) value));
                } else {
                    // todo 类型还需充分枚举
                    newRow.setValue(index, ValueString.get(value.toString()));
                }
            }
        #+END_SRC

- 提交插入
    因为从 *org.h2.engine.Session* 剥离出了Table对象，而h2是支持事务的数据库，所以在插入结束后，还需要执行commit，让改变生效。

        #+BEGIN_SRC java
            session.commit(false);
        #+END_SRC

*** 最终效果
2.7w 条数据，耗时 700ms。相比传统方案(2.7w条数据，3000ms)，耗时减少了将近八成，颇为可观了。

*** 源码

        #+BEGIN_SRC 

import lombok.extern.slf4j.Slf4j;
import org.h2.command.CommandContainer;
import org.h2.command.dml.Select;
import org.h2.engine.Session;
import org.h2.jdbc.JdbcConnection;
import org.h2.jdbc.JdbcPreparedStatement;
import org.h2.result.Row;
import org.h2.table.Column;
import org.h2.table.Table;
import org.h2.value.*;
import org.springframework.util.ReflectionUtils;

import java.lang.reflect.Field;
import java.math.BigDecimal;
import java.sql.Connection;
import java.sql.SQLException;
import java.sql.Timestamp;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.TimeZone;

@Slf4j
public class H2InsertUtil {

    public static void batchInsert(Connection toSqlSession, String tableName, List<Map<String, Object>> data) {
        assert isH2Dialect(toSqlSession);

        try {
            JdbcConnection connection = toSqlSession.unwrap(JdbcConnection.class);
            doBatchInsert(connection, tableName, data);
        } catch (SQLException e) {
            throw new RuntimeException("使用H2批量插入出错.", e);
        }
    }

    private static boolean isH2Dialect(Connection sqlSession) {
        try {
            return sqlSession.isWrapperFor(JdbcConnection.class);
        } catch (SQLException e) {
            log.warn("判断connection类型时出错", e);
            return false;
        }
    }

    private static void doBatchInsert(JdbcConnection connection, String tableName, List<Map<String, Object>> batchData) throws SQLException {
        String sql = "select * from " + tableName;
        try (JdbcPreparedStatement ps = (JdbcPreparedStatement) connection.prepareStatement(sql)) {
            CommandContainer commandContainer = (CommandContainer) getFieldByForce(ps, JdbcPreparedStatement.class,
                    "command");
            Session session = (Session) getFieldByForce(ps, JdbcPreparedStatement.class, "session");
            Select command = (Select) getFieldByForce(commandContainer, CommandContainer.class, "prepared");
            Table table = new ArrayList<>(command.getTables()).get(0);

            for (Map<String, Object> data : batchData) {
                Row newRow = createRow(table, data);
                table.addRow(session, newRow);
            }
            session.commit(false);
        } catch (Exception e) {
            log.error("", e);
            throw e;
        }
    }

    private static Object getFieldByForce(Object obj, Class<?> clazz, String fieldName) {
        Field field = ReflectionUtils.findField(clazz, fieldName);
        ReflectionUtils.makeAccessible(field);
        return ReflectionUtils.getField(field, obj);
    }

    private static Row createRow(Table table, Map<String, Object> map) {
        Row newRow = table.getTemplateRow();
        Column[] columns = table.getColumns();
        for (Column c : columns) {
            int index = c.getColumnId();
            String columnName = c.getName();
            if (!map.containsKey(columnName)) {
                newRow.setValue(c.getColumnId(), ValueNull.INSTANCE);
            } else {
                Object value = map.get(columnName);
                if (value instanceof String) {
                    newRow.setValue(index, ValueString.get(value.toString()));
                } else if (value instanceof Integer) {
                    newRow.setValue(index, ValueInt.get((Integer) value));
                } else if (value instanceof Timestamp) {
                    newRow.setValue(index, ValueTimestamp.get(TimeZone.getDefault(), (Timestamp) value));
                } else if (value instanceof BigDecimal) {
                    newRow.setValue(index, ValueDecimal.get((BigDecimal) value));
                } else {
                    // todo 类型还需充分枚举
                    newRow.setValue(index, ValueString.get(value.toString()));
                }
            }
        }
        return newRow;
    }
}
        #+END_SRC

** 最近一次重装系统                                                  :Emacs:
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_DATE: "2021-09-12T15:37:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2021-09-12T15:48:52+08:00"
:EXPORT_FILE_NAME: reinstall-windows-10-for-dual-boot-system
:EXPORT_DESCRIPTION: 最新一次重装Windows10
:END:
上次重装系统是什么时候？很久很久之前了吧。   

手上这台二手Thinkpad T450s，装着 ~Ubuntu~ 和 ~Windows 10~ ，其中 ~Ubuntu~ 是我装的; ~Windows 10~ 继承自上任主人，我接手后几乎没有"装修"，一直凑合用着。最近，它越来越不趁手，终于我下定决心——重装Windows。

*** 镜像与烧录工具
根据之前捣腾 ~Ubuntu~ 的经验，需要准备三件东西:
- 系统镜像
- 将镜像烧录 U 盘的工具软件
- U盘，用作启动盘

哪个 ~Windows~ 版本最好？在知乎网搜索一番，很多网友推荐 ~Windows Enterprise LTSC 2019~ ，而且推荐一个神奇的下载网站 [[https://msdn.itellyou.cn/][I Tell You]], 上面有各种各样镜像资源。LTSC 镜像下载链接是 `ed2k` 格式，依稀记得是一种BT下载协议（？），现在比较少见。我不得不先安装迅雷，再用迅雷处理这个小众的 `ed2k` 链接。说句题外话，迅雷居然还活着，实在令人震惊，但接着发现它趁我不备偷偷装上了一个奇怪软件，又有些唏嘘了:国产软件活着不容易啊，就连迅雷也堕落成流氓软件了！

镜像烧录工具，知乎网友推荐用 https://rufus.ie/zh/ 。

U盘，我随手找到一个8G盘。

这时候，准备工作完毕。

*** 重启电脑，开始重装
说下前情提要。这台 T450s 装过双系统，启动方案已经是 ~UEFI~ ，上电后默认进入 Ubuntu Grub 引导，可以选择进入 Ubuntu 还是 Windows。(还记得昔日主流启动引导方案 ~MBR~ 吗？现在已经称作 ~Legacy/MBR~ 。真是历史的眼泪啊。)

这里不详述 ~UEFI/GPT~ 的先进性。在支持双系统启动这一点上， ~UEFI/GPT~ 比起 ~UEFI/GPT~  要复杂一些: 需要修改 ~BIOS~ 菜单，将 ~Secure Boot~ 置为 ~false~ 。具体可以参考[[https://support.lenovo.com/hk/en/solutions/ht509044-how-to-enable-secure-boot-on-think-branded-systems-thinkpad-thinkstation-thinkcentre][官方文档]]。
 
我的T450s已经配置过 ~BIOS~ ，所以跳过上面这步。

进入开机启动，眼疾手快，迅速按 ~F12~ 进入启动菜单。选择从U盘启动，进入安装界面。

*** 新系统升级驱动
系统装好后，总感觉清晰度不够。我猜想，是驱动不适配。搜索 ~驱动人生~ 关键字，这是类似驱动精灵的一款软件。为什么不直接安装驱动精灵？因为驱动精灵也长成流氓软件的模样了。。 ~驱动人生~ 实际体验还可以，有点古典互联网软件那意思。

驱动人生识别出系统显卡驱动需要升级。选自动升级，然后屏幕恢复正常。

*** 调整 BIOS 引导顺序
重启电脑，如果什么都不做，将默认进入 Windows10。原因是，Windows10 装好后自动生成一条 Windows 引导记录，而且优先级最高。之前 ~Ubuntu~ 的引导项仍然在那儿，只是往后挪了一位。电脑启动时，总是尝试按优先级加载，自然进入 Windows 。

要找回 Linux GRUB 引导界面，只需进入 ~BIOS~ 菜单，找到设置引导顺序的界面，把 Linux 引导条目挪到第一位。

*** 小问题：双系统，Windows 时间对不上？
原因是 Windows 和 Linux 处理硬件时间的方式不一样。参考网友的[[https://blog.csdn.net/zhouchen1998/article/details/108893660][解决方案]], 用三行命令解决问题:

#+BEGIN_SRC  bash
  sudo apt install ntpdate;
  sudo ntpdate time.windows.com;
  sudo hwclock --localtime --systohc
#+END_SRC 
    
* Golang is great                                                               :Golang:
* Cryptography
** 浏览器验证SSL数字证书的步骤
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: how_do_web_broswer_validate_ssl_certificates
:EXPORT_DESCRIPTION: 本文介绍通过https协议访问网站时，浏览器验证服务器SSL证书的原理，并以Chrome访问百度为例进行分析。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "ssl" )
:END:

浏览器和服务器使用SSL/TLS通信时，双方首先要通过几次握手(Handshake)，建立加密信道。简单说来，分为下面３步:
1. 服务器发送自己的SSL证书；
2. 浏览器验证服务器SSL证书；
3. 证书验证成功，双方协商得到对称加密密钥，并交换。双方拿到对称加密密钥后，后续的通信都会用它做对称加密。

本文介绍的重点，在前２步。首先，转载一篇国外博客，讲述浏览器检查证书的过程；其次，会引述两个RFC协议的相关内容；最后，wireshark抓包进行验证。

*** Browsers and Certificate Validation
原文地址: https://www.ssl.com/article/browsers-and-certificate-validation/

使用[[https://www.deepl.com/en/translator][DeepL]] 翻译成中文，如下:

#+BEGIN_SRC 
## 证书和X.509格式
证书在各方面都是数字文件，这意味着它们需要遵循一种文件格式来存储信息（如签名、密钥、签发人等）。虽然私有的PKI配置可以为其证书实现任何格式，但公共信任的PKIs（即那些被浏览器信任的PKIs）必须符合RFC 5280，这就要求使用X.509 v3格式。

X.509 v3允许证书包含额外的数据，如使用限制或策略信息，作为扩展，每个扩展都是关键或非关键的。浏览器可以忽略无效的或未被识别的非关键扩展，但它们必须处理和验证所有关键扩展。

## 认证路径和路径处理
憑證機構使用私人密碼匙對所有簽發的證書進行加密簽署。这种签名可以不可撤销地证明证书是由某一特定的核证机 构签发的，而且在签署后没有被修改。

CA通过持有相应公钥的自发证书（称为根）来建立其签名密钥的所有权。憑證機構必須遵守嚴格的控制和審核程序來建立、管理和使用根證書，為了減少暴露，通常會使用根證書來簽發中間證書。这些中间证书可以用来签发客户的证书。
浏览器在出厂时都有一个内置的可信根列表。(这些根是来自通过浏览器严格标准的CA的根。) 为了验证证书，浏览器将获得一个证书序列，每个证书都签署了序列中的下一个证书，将签名CA的根与服务器的证书连接起来。

这个证书序列称为认证路径。路径的根部称为信任锚，服务器的证书称为叶子或终端实体证书。

### 路径的构造
通常情况下，浏览器必须考虑多个认证路径，直到他们能够为给定证书找到一个有效的路径。即使一个路径可能包含的证书可以正确地 "链 "到一个已知的锚，但由于路径长度、域名、证书使用或政策的限制，路径本身可能会被拒绝。

对于浏览器遇到的每一个新证书，构建和评估所有可能的路径都是一个昂贵的过程。浏览器已经实现了各种优化，以减少被拒绝的候选路径的数量，但深入探讨这些细节已经超出了本文的范围。

### 路径验证
候选认证路径构建完成后，浏览器使用证书中包含的信息对其进行验证。如果浏览器能够通过密码学的方式证明，从一个信任锚直接签署的证书开始，每个证书对应的私钥都被用来签发路径中的下一个证书，一直到叶子证书，那么这个路径就是有效的。

## 认证路径验证算法
RFC 5280描述了浏览器验证X.509证书认证路径的标准算法。

基本上，浏览器从信任锚（即根证书）开始，遍历路径中的所有证书，验证每张证书的基本信息和关键扩展。

如果该过程以路径中的最后一张证书结束，没有错误，那么该路径被接受为有效。如果产生错误，则该路径被标记为无效。

### 证书的基本处理
无论是否有任何扩展，浏览器必须始终验证基本的证书信息，如签名或签发人。下面的章节显示了浏览器执行检查的顺序。

1. 浏览器验证证书的完整性
证书上的签名可以用正常的公用钥匙加密法进行验证。如果签名无效，则认为该证书在签发后被修改，因此被拒绝。

2. 浏览器验证证书的有效性：
證書的有效期是指簽署憑證機構保證會維持其狀態資訊的時間間隔。浏览器会拒绝任何有效期在验证检查日期和时间之前或之后开始的证书。

3. 浏览器检查证书的撤销状态。
证书签发后，应该在整个有效期内使用。当然，在各种情况下，证书可能会在自然到期前失效。

这类情况可能包括主体改名或怀疑私钥泄露。在这样的情况下，CA需要撤销相应的证书，而用户也信任CA会通知浏览器其证书的撤销状态。

RFC 5280建议CA使用撤销列表来实现这一目的。

证书废止列表(CRL)
核證機關會定期發出一份經簽署、有時間標記的廢止證書清單，稱為證書廢止清單（CRL）。CRL分布在公开的存储库中，浏览器在验证证书时可以获得并查阅CA的最新CRL。

这种方法的一个缺陷是，撤销的时间粒度仅限于CRL的发布期。只有在所有当前已发布的CRL都被安排更新后，浏览器才会收到撤销的通知。根据签名CA的政策，这可能需要一个小时、一天甚至一周的时间。

在线证书状态协议(OCSP)
还有其他的方法来获取废止状态信息，其中最流行的是在线证书状态协议（OCSP）。

OCSP在标准文档RFC6960中进行了描述，它允许浏览器从在线OCSP服务器（也称为回复者）请求特定证书的撤销状态。如果配置得当，OCSP的即时性更强，而且避免了上面提到的CRL更新延迟问题。此外，OCSP Stapling还能提高性能和速度。

4. 浏览器验证发件人
证书通常与两个实体相关联。

签发人，也就是拥有签名密钥的实体，以及
主体，指的是证书认证的公钥的所有者。
浏览器会检查证书的签发人字段是否与路径中前一个证书的主题字段相同。为了增加安全性，大多数PKI实现也会验证发证者的密钥是否与签署当前证书的密钥相同。(请注意，这对于信任锚来说并不正确，因为根是自发的--即它们具有相同的签发人和主体)。

约束处理
X.509 v3格式允许CA定义约束或限制每个证书如何被验证和作为关键扩展使用。路径中的每张证书都可以施加额外的约束，所有后续证书都必须遵守。

证书约束很少影响普通互联网用户，尽管它们在企业SSL解决方案中相当常见。功能性约束可以达到多种操作目的，但其最重要的用途是缓解已知的安全问题。

5. 浏览器检查名称约束
具有适当名称限制的私有(但公众信任的)中间CA可以为组织提供对证书管理和签发的精细控制。证书可以被限制在一个公司或组织的域名的特定域或域树（即包括子域）。名称限制通常用于从公开信任的CA购买的中间CA证书，以防止中间CA为第三方域（如google.com）签发完全有效的证书。

6. 浏览器检查策略约束
證書政策是由核證機關所發表的法律文件，正式詳述其簽發及管理證書的程序。憑證機構可以根據一項或多項政策簽發證書，而每張證書都有這些政策的連結，以便信賴者在決定信任該證書前，可以評估這些政策。

出于法律和操作上的原因，证书可以对证书的政策进行限制。如果发现证书中包含关键策略约束，浏览器必须在进行之前对其进行验证。(然而，关键策略约束在现实世界中很少遇到，所以本文其余部分将不予考虑)。

7. 浏览器检查基本约束（也就是路径长度）。
X.509 v3格式允许签发人定义证书所能支持的最大路径长度。这提供了对每个证书在认证路径中可以放置多远的控制。这实际上是很重要的--浏览器曾经无视认证路径长度，直到一位研究人员在2009年的一次演讲中演示了他如何利用自己网站的叶子证书为一个大型电子商务网站伪造有效证书。

8. 浏览器验证密钥用途
钥匙用途 "扩展部分说明了证书中钥匙的用途，例如加密、签名、证书签名等。这些用途的例子包括加密、签名、证书签名等。浏览器会拒绝违反其密钥用途限制的证书，例如遇到服务器证书的密钥只用于CRL签名。

9. 浏览器继续处理所有剩余的关键扩展文件
浏览器在处理完上述扩展证书后，会继续验证当前证书指定为关键的所有剩余扩展证书，然后再进入下一个。如果浏览器到达一个路径的叶子证书时没有错误，那么该路径就会被接受为有效。如果产生任何错误，则路径被标记为无效，并且不能建立安全连接。

通过www.DeepL.com/Translator（免费版）翻译
 #+END_SRC

*** 两个重要RFC标准
上面转载的文章，详细讲述了浏览器的验证过程。细心人会发现，文章多次提到RFC 5280。这个标准定义了X.509证书格式，是互联网加密体系中处于基石地位的标准之一。除了RFC 5280，还有一个比较重要的标准 RFC 5246，定义了SSL传输层协议。

**** [RFC 5280] PKI X.509 v3规范　
https://tools.ietf.org/html/rfc5280   

***** 路径验证算法
其中，section-6 给出了证书validation算法，并给出了路径验证算法（section-6）。
在此，摘录关键部分: 
#+BEGIN_SRC  text
 (a)  for all x in {1, ..., n-1}, the subject of certificate x is
           the issuer of certificate x+1;

 (b)  certificate 1 is issued by the trust anchor;

 (c)  certificate n is the certificate to be validated (i.e., the
      target certificate); and

 (d)  for all x in {1, ..., n}, the certificate was valid at the
           time in question
#+END_SRC 

根据算法，第一个证书由trust anchor签发，下一个证书由这个证书签发……直到最后的叶子节点证书。这样由信任锚长出一个链条，一环扣一环，链条上每个节点都是可信的。

***** 证书指纹(fingerprint)验证算法
x509如何计算fingerprint?
https://stackoverflow.com/questions/4803799/how-to-calculate-x-509-certificates-sha-1-fingerprint

生成过程: 1) 根据signature算法，计算出证书TBS部分的signature 2) 证书签发者issuer用自己的私钥加密signature，得到fingerprint
验证过程: 1) 根据signature算法，计算出证书TBS部分的signature 2) 使用证书签发者issuer的公钥解密fingerprint, 得到signature 3) 比对两个signature

**** [RFC 5246] TLS 1.2规范
https://tools.ietf.org/html/rfc5246   

其中，section-7.4.2　规定，server要向client发送 /certificate_list/ 。服务器不是返回单独的某个证书，而是一个证书列表; 因为单独一个证书，没法形成certifate chain，也就无法完成validation: 这和[RFC 5280] 所述流程吻合。 

#+BEGIN_SRC text  
certificate_list
      This is a sequence (chain) of certificates.  The sender's
      certificate MUST come first in the list.  Each following
      certificate MUST directly certify the one preceding it.  Because
      certificate validation requires that root keys be distributed
      independently, the self-signed certificate that specifies the root
      certificate authority MAY be omitted from the chain, under the
      assumption that the remote end must already possess it in order to
      validate it in any case.
#+END_SRC 

这里指定了证书的顺序，第一个是叶子证书。很好理解，因为重要数据在报文中的位置往往靠前。

*** 抓包看看　
借助wireshark，我们实际操作一番。  

打开wireshark, 开始抓包，再访问百度官网(https://www.baidu.com)。抓包细节如下：　

[[file:e:/blog/static/ox-hugo/ssl_certificate_wireshark.png][wireshark抓包分析]]


** 手工验证一张数字证书的有效性
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: validate_a_digital_certificate_step_by_step
:EXPORT_DESCRIPTION: 尽可能细致地实践证书验证算法
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "ssl" "CA" )
:END:
上一篇 [[https://blog.yuantops.com/tech/how_do_web_broswer_validate_ssl_certificates/][博客]] 讨论浏览器验证数字证书的流程。这篇文章更深入一步，用原始方法一步步手工验证证书的合法性。本文主要参考: [[https://security.stackexchange.com/questions/127095/manually-walking-through-the-signature-validation-of-a-certificate][回答]] 与 [[https://segmentfault.com/a/1190000019008423][X.509、PKCS文件格式介绍]]。

*** 基础名词
**** ASN.1, DER与PEM
ASN.1是一种接口描述语言，它用来定义一种数据结构。

DER是一种编码规则，它用二进制表示ASN.1定义的数据。很多密码学标准使用ASN.1定义数据结构，用DER编码。

但因为DER的内容是二进制的，不方便传输，人们对DER二进制内容进行Base64编码，将其转换为ASCII码，并在头和尾加上标签，就是PEM格式。PEM全称Privacy-Enhanced Mail，起初是为了便于邮件传输，后来在很多场景得到广泛应用。

**** X.509
X.509是RFC5280定义的一种公钥证书格式(public key certificate)。X.509证书也被称为数字Digital Certificate。一张X.509包含一个Public Key和一个身份信息。X.509证书要么是自签发，要么是被CA签发。

*** 如何得到一张证书
借助浏览器，可以方便导出数字证书。

打开chrome，访问本博客网址(https://blog.yuantops.com)，地址栏最左侧有个小锁图案 —— 这是网站受到HTTPS加密保护的标志。

在"Details"标签，观察"Certificate Subject Alternative Name"字段，值包含"DNS Name: yuantops.com" "DNS　Name: *.yuantops.com"，说明证书的确属于这个域名。

点击小锁　-> "certificate" -> "Details" -> "Export..."，可以选择证书的导出格式。

选择"Base64-encoded ASCII, single certificate"，得到一张PEM格式证书。将它保存为`sni.cloudflaressl.com`。
#+BEGIN_SRC txt
-----BEGIN CERTIFICATE-----
MIIEwzCCBGmgAwIBAgIQDVZy4W9/IjNEOZEGQ2ADTjAKBggqhkjOPQQDAjBKMQsw
CQYDVQQGEwJVUzEZMBcGA1UEChMQQ2xvdWRmbGFyZSwgSW5jLjEgMB4GA1UEAxMX
Q2xvdWRmbGFyZSBJbmMgRUNDIENBLTMwHhcNMjAwODA3MDAwMDAwWhcNMjEwODA3
MTIwMDAwWjBtMQswCQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNh
biBGcmFuY2lzY28xGTAXBgNVBAoTEENsb3VkZmxhcmUsIEluYy4xHjAcBgNVBAMT
FXNuaS5jbG91ZGZsYXJlc3NsLmNvbTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IA
BCh3/Sz4YWHFP32cBLzErjTKy4/AdFKU37wFK8kzP7sdhM3/BxdJNKeRYNwcDimw
k76zgHaaGki0AzvCTMa+llWjggMMMIIDCDAfBgNVHSMEGDAWgBSlzjfq67B1DpRn
iLRF+tkkEIeWHzAdBgNVHQ4EFgQUi9pqgIAX5apgTXwOGZ9k1FALDL0wPgYDVR0R
BDcwNYIOKi55dWFudG9wcy5jb22CFXNuaS5jbG91ZGZsYXJlc3NsLmNvbYIMeXVh
bnRvcHMuY29tMA4GA1UdDwEB/wQEAwIHgDAdBgNVHSUEFjAUBggrBgEFBQcDAQYI
KwYBBQUHAwIwewYDVR0fBHQwcjA3oDWgM4YxaHR0cDovL2NybDMuZGlnaWNlcnQu
Y29tL0Nsb3VkZmxhcmVJbmNFQ0NDQS0zLmNybDA3oDWgM4YxaHR0cDovL2NybDQu
ZGlnaWNlcnQuY29tL0Nsb3VkZmxhcmVJbmNFQ0NDQS0zLmNybDBMBgNVHSAERTBD
MDcGCWCGSAGG/WwBATAqMCgGCCsGAQUFBwIBFhxodHRwczovL3d3dy5kaWdpY2Vy
dC5jb20vQ1BTMAgGBmeBDAECAjB2BggrBgEFBQcBAQRqMGgwJAYIKwYBBQUHMAGG
GGh0dHA6Ly9vY3NwLmRpZ2ljZXJ0LmNvbTBABggrBgEFBQcwAoY0aHR0cDovL2Nh
Y2VydHMuZGlnaWNlcnQuY29tL0Nsb3VkZmxhcmVJbmNFQ0NDQS0zLmNydDAMBgNV
HRMBAf8EAjAAMIIBBAYKKwYBBAHWeQIEAgSB9QSB8gDwAHYA9lyUL9F3MCIUVBgI
MJRWjuNNExkzv98MLyALzE7xZOMAAAFzyS9NoAAABAMARzBFAiB5au5KCRfkyBcI
7jECy/NvNPkKEoMUUTwZP+rZbHtn8AIhAKOR2Lh2zsCw+gy38abKie1fyd1rmm0c
GA/pP6PykChvAHYAXNxDkv7mq0VEsV6a1FbmEDf71fpH3KFzlLJe5vbHDsoAAAFz
yS9N0wAABAMARzBFAiALkQMvm51FKVO2JRFiWWEgqu4x9rGHy2JH6P2m18lrLQIh
AN1PcRtCiY+gihkncncx18OZM6e5CGZruk05EDGThLTvMAoGCCqGSM49BAMCA0gA
MEUCIHXeLOwERMHY88NliKhUzs1MwoJap9sNm9qQLGXYCpEMAiEA1ZsGvWxusXK9
tAgwUjlWi5Ke5rvM/i01sYl6bpls4Z0=
-----END CERTIFICATE-----
#+END_SRC

*** 分析证书结构
RFC5280规定了X.509证书的语法:
#+BEGIN_SRC txt
   Certificate  ::=  SEQUENCE  {
        tbsCertificate       TBSCertificate,
        signatureAlgorithm   AlgorithmIdentifier,
        signatureValue       BIT STRING  }
#+END_SRC

根据定义,证书由TBSCertificate, 签名算法，签名值三部分构成。 我们可以将它们分别提取出来。提取之前，先观察证书结构:

#+BEGIN_SRC shell
openssl asn1parse -i -in sni.cloudflaressl.com 
#+END_SRC
***** 选项解释
`-in filename`: 输入文件名

`-i`: 标记实体，输出缩进标记，将一个ASN1实体下的其他对象缩进显示。此选项非默认选项，加上此选项后，显示更易看懂。
***** 输出
#+BEGIN_SRC txt
    0:d=0  hl=4 l=1219 cons: SEQUENCE          
    4:d=1  hl=4 l=1129 cons:  SEQUENCE          
    8:d=2  hl=2 l=   3 cons:   cont [ 0 ]        
   10:d=3  hl=2 l=   1 prim:    INTEGER           :02
   13:d=2  hl=2 l=  16 prim:   INTEGER           :0D5672E16F7F2233443991064360034E
   31:d=2  hl=2 l=  10 cons:   SEQUENCE          
   33:d=3  hl=2 l=   8 prim:    OBJECT            :ecdsa-with-SHA256
   43:d=2  hl=2 l=  74 cons:   SEQUENCE          
   45:d=3  hl=2 l=  11 cons:    SET               
   47:d=4  hl=2 l=   9 cons:     SEQUENCE          
   49:d=5  hl=2 l=   3 prim:      OBJECT            :countryName
   54:d=5  hl=2 l=   2 prim:      PRINTABLESTRING   :US
   58:d=3  hl=2 l=  25 cons:    SET               
   60:d=4  hl=2 l=  23 cons:     SEQUENCE          
   62:d=5  hl=2 l=   3 prim:      OBJECT            :organizationName
   67:d=5  hl=2 l=  16 prim:      PRINTABLESTRING   :Cloudflare, Inc.
   85:d=3  hl=2 l=  32 cons:    SET               
   87:d=4  hl=2 l=  30 cons:     SEQUENCE          
   89:d=5  hl=2 l=   3 prim:      OBJECT            :commonName
   94:d=5  hl=2 l=  23 prim:      PRINTABLESTRING   :Cloudflare Inc ECC CA-3
  119:d=2  hl=2 l=  30 cons:   SEQUENCE          
  121:d=3  hl=2 l=  13 prim:    UTCTIME           :200807000000Z
  136:d=3  hl=2 l=  13 prim:    UTCTIME           :210807120000Z
  151:d=2  hl=2 l= 109 cons:   SEQUENCE          
  153:d=3  hl=2 l=  11 cons:    SET               
  155:d=4  hl=2 l=   9 cons:     SEQUENCE          
  157:d=5  hl=2 l=   3 prim:      OBJECT            :countryName
  162:d=5  hl=2 l=   2 prim:      PRINTABLESTRING   :US
  166:d=3  hl=2 l=  11 cons:    SET               
  168:d=4  hl=2 l=   9 cons:     SEQUENCE          
  170:d=5  hl=2 l=   3 prim:      OBJECT            :stateOrProvinceName
  175:d=5  hl=2 l=   2 prim:      PRINTABLESTRING   :CA
  179:d=3  hl=2 l=  22 cons:    SET               
  181:d=4  hl=2 l=  20 cons:     SEQUENCE          
  183:d=5  hl=2 l=   3 prim:      OBJECT            :localityName
  188:d=5  hl=2 l=  13 prim:      PRINTABLESTRING   :San Francisco
  203:d=3  hl=2 l=  25 cons:    SET               
  205:d=4  hl=2 l=  23 cons:     SEQUENCE          
  207:d=5  hl=2 l=   3 prim:      OBJECT            :organizationName
  212:d=5  hl=2 l=  16 prim:      PRINTABLESTRING   :Cloudflare, Inc.
  230:d=3  hl=2 l=  30 cons:    SET               
  232:d=4  hl=2 l=  28 cons:     SEQUENCE          
  234:d=5  hl=2 l=   3 prim:      OBJECT            :commonName
  239:d=5  hl=2 l=  21 prim:      PRINTABLESTRING   :sni.cloudflaressl.com
  262:d=2  hl=2 l=  89 cons:   SEQUENCE          
  264:d=3  hl=2 l=  19 cons:    SEQUENCE          
  266:d=4  hl=2 l=   7 prim:     OBJECT            :id-ecPublicKey
  275:d=4  hl=2 l=   8 prim:     OBJECT            :prime256v1
  285:d=3  hl=2 l=  66 prim:    BIT STRING        
  353:d=2  hl=4 l= 780 cons:   cont [ 3 ]        
  357:d=3  hl=4 l= 776 cons:    SEQUENCE          
  361:d=4  hl=2 l=  31 cons:     SEQUENCE          
  363:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Authority Key Identifier
  368:d=5  hl=2 l=  24 prim:      OCTET STRING      [HEX DUMP]:30168014A5CE37EAEBB0750E946788B445FAD9241087961F
  394:d=4  hl=2 l=  29 cons:     SEQUENCE          
  396:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Subject Key Identifier
  401:d=5  hl=2 l=  22 prim:      OCTET STRING      [HEX DUMP]:04148BDA6A808017E5AA604D7C0E199F64D4500B0CBD
  425:d=4  hl=2 l=  62 cons:     SEQUENCE          
  427:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Subject Alternative Name
  432:d=5  hl=2 l=  55 prim:      OCTET STRING      [HEX DUMP]:3035820E2A2E7975616E746F70732E636F6D8215736E692E636C6F7564666C61726573736C2E636F6D820C7975616E746F70732E636F6D
  489:d=4  hl=2 l=  14 cons:     SEQUENCE          
  491:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Key Usage
  496:d=5  hl=2 l=   1 prim:      BOOLEAN           :255
  499:d=5  hl=2 l=   4 prim:      OCTET STRING      [HEX DUMP]:03020780
  505:d=4  hl=2 l=  29 cons:     SEQUENCE          
  507:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Extended Key Usage
  512:d=5  hl=2 l=  22 prim:      OCTET STRING      [HEX DUMP]:301406082B0601050507030106082B06010505070302
  536:d=4  hl=2 l= 123 cons:     SEQUENCE          
  538:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 CRL Distribution Points
  543:d=5  hl=2 l= 116 prim:      OCTET STRING      [HEX DUMP]:30723037A035A0338631687474703A2F2F63726C332E64696769636572742E636F6D2F436C6F7564666C617265496E6345434343412D332E63726C3037A035A0338631687474703A2F2F63726C342E64696769636572742E636F6D2F436C6F7564666C617265496E6345434343412D332E63726C
  661:d=4  hl=2 l=  76 cons:     SEQUENCE          
  663:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Certificate Policies
  668:d=5  hl=2 l=  69 prim:      OCTET STRING      [HEX DUMP]:3043303706096086480186FD6C0101302A302806082B06010505070201161C68747470733A2F2F7777772E64696769636572742E636F6D2F4350533008060667810C010202
  739:d=4  hl=2 l= 118 cons:     SEQUENCE          
  741:d=5  hl=2 l=   8 prim:      OBJECT            :Authority Information Access
  751:d=5  hl=2 l= 106 prim:      OCTET STRING      [HEX DUMP]:3068302406082B060105050730018618687474703A2F2F6F6373702E64696769636572742E636F6D304006082B060105050730028634687474703A2F2F636163657274732E64696769636572742E636F6D2F436C6F7564666C617265496E6345434343412D332E637274
  859:d=4  hl=2 l=  12 cons:     SEQUENCE          
  861:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Basic Constraints
  866:d=5  hl=2 l=   1 prim:      BOOLEAN           :255
  869:d=5  hl=2 l=   2 prim:      OCTET STRING      [HEX DUMP]:3000
  873:d=4  hl=4 l= 260 cons:     SEQUENCE          
  877:d=5  hl=2 l=  10 prim:      OBJECT            :CT Precertificate SCTs
  889:d=5  hl=3 l= 245 prim:      OCTET STRING      [HEX DUMP]:0481F200F0007600F65C942FD1773022145418083094568EE34D131933BFDF0C2F200BCC4EF164E300000173C92F4DA000000403004730450220796AEE4A0917E4C81708EE3102CBF36F34F90A128314513C193FEAD96C7B67F0022100A391D8B876CEC0B0FA0CB7F1A6CA89ED5FC9DD6B9A6D1C180FE93FA3F290286F0076005CDC4392FEE6AB4544B15E9AD456E61037FBD5FA47DCA17394B25EE6F6C70ECA00000173C92F4DD3000004030047304502200B91032F9B9D452953B6251162596120AAEE31F6B187CB6247E8FDA6D7C96B2D022100DD4F711B42898FA08A1927727731D7C39933A7B908666BBA4D3910319384B4EF
 1137:d=1  hl=2 l=  10 cons:  SEQUENCE          
 1139:d=2  hl=2 l=   8 prim:   OBJECT            :ecdsa-with-SHA256
 1149:d=1  hl=2 l=  72 prim:  BIT STRING        
 #+END_SRC

***** 输出格式解析
#+BEGIN_SRC 
    0:d=0  hl=4 l=1219 cons: SEQUENCE          
 #+END_SRC

`0`: 表示节点在整个文件中的偏移长度
`d=0`: 表示节点深度
`hl=4`: 表示节点头字节长度
`l=1219`: 表示节点数据字节长度
`cons`: 表示该节点为结构节点，表示包含子节点或者子结构数据
`prim`: 表示该节点为原始节点，包含数据

***** tbsCertificate和signature位置
观察可知，tbsCertificate的偏移位置是4, 签名值signatureValue的偏移位置是1137。

*** 提取tbsCertificate
引用RFC5280 4.1.1.3:
#+BEGIN_SRC txt
   The signatureValue field contains a digital signature computed upon
   the ASN.1 DER encoded tbsCertificate.  The ASN.1 DER encoded
   tbsCertificate is used as the input to the signature function. 
#+END_SRC

根据上述定义，计算签名的输入是DER编码的tbsCertificate。而我们从浏览器导出的证书是PEM格式，需要使用openssl将其转化为DER格式。
#+BEGIN_SRC 
openssl x509 -in sni.cloudflaressl.com -inform PEM -out sni.cloudflaressl.com.der -outform DER
#+END_SRC

然后，我们从DER证书中提取tbsCertificate。 根据asn1parse输出结果第二行，tbsCertificate偏移位置是4, 大小是1133 = ４(头部长度) + 1129(数据长度)。
#+BEGIN_SRC 
    4:d=1  hl=4 l=1129 cons:  SEQUENCE          
#+END_SRC

使用`dd` 按偏移位置截取。输出内容保存到`yuantops.tbs`。
#+BEGIN_SRC 
dd if=sni.cloudflaressl.com.der of=yuantops.tbs skip=4 bs=1 count=1133
#+END_SRC

*** 提取signatureValue
根据asn1parse输出结果末尾一行:

#+BEGIN_SRC 
 1149:d=1  hl=2 l=  72 prim:  BIT STRING        
#+END_SRC

signatureValue偏移量是1137。如果直接使用`dd`截取，将得到`BIT STRING`编码后的签名值，不能直接使用。需要用`ans1parse`的`-strparse`选项，将其转换为二进制数据。

#+BEGIN_SRC shell
 openssl asn1parse -in sni.cloudflaressl.com -strparse 1137 -out cloudflaressl.sig
#+END_SRC

签名数据保存在cloudflaressl.sig文件。

***** 选项解释
`-in filename` ：输入文件名，默认为标准输入。

`-offset number`：开始数据分析的字节偏移量，分析数据时，不一定从头开始分析，可用指定偏移量，默认从头开始分析。

`-strparse offset`：此选项也用于从一个偏移量开始来分析数据，不过，与-offset不一样。-offset分析偏移量之后的所有数据，而-strparse只用于分析一段数据，并且这种数据必须是SET或者SEQUENCE，它只分析本SET或者SEQUENCE范围的数据。

***** 查看提取的签名是否正确

使用`od`命令，以16进制打印签名文件内容: 
#+BEGIN_SRC 
`od -tx1 cloudflaressl.sig`
0000000 30 45 02 20 75 de 2c ec 04 44 c1 d8 f3 c3 65 88
0000020 a8 54 ce cd 4c c2 82 5a a7 db 0d 9b da 90 2c 65
0000040 d8 0a 91 0c 02 21 00 d5 9b 06 bd 6c 6e b1 72 bd
0000060 b4 08 30 52 39 56 8b 92 9e e6 bb cc fe 2d 35 b1
0000100 89 7a 6e 99 6c e1 9d
0000107
#+END_SRC
与浏览器Certificate Viewer中看到的证书`SignatureValue`对比，二者应该相同。

下一步，我们获取签发者的公钥。
*** 获取issuer公钥
在上一篇博客中提到，服务器返回给浏览器一组证书链。通过浏览器Certificate Viewer可以看到证书继承关系。 `sni.cloudflaressl.com`证书的签发者是`Cloudflare Inc ECC CA-3`。

我们将其导出为文件，保存到本地，文件名为 `Cloudflare_Inc_ECC_CA-3`。

从证书提取公钥:
#+BEGIN_SRC 
openssl x509 -in Cloudflare_Inc_ECC_CA-3 -noout -pubkey > Cloudflare_Inc_ECC_CA-3.pub
#+END_SRC

如果想观察公钥内容，可以将其转换为PEM格式:
#+BEGIN_SRC text
openssl pkey -in Cloudflare_Inc_ECC_CA-3.pub -pubin -text

-----BEGIN PUBLIC KEY-----
MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEua1NZpkUC0bsH4HRKlAenQMVLzQS
fS2WuIg4m4Vfj7+7Te9hRsTJc9QkT+DuHM5ss1FxL2ruTAUJd9NyYqSb1w==
-----END PUBLIC KEY-----
Public-Key: (256 bit)
pub: 
    04:b9:ad:4d:66:99:14:0b:46:ec:1f:81:d1:2a:50:
    1e:9d:03:15:2f:34:12:7d:2d:96:b8:88:38:9b:85:
    5f:8f:bf:bb:4d:ef:61:46:c4:c9:73:d4:24:4f:e0:
    ee:1c:ce:6c:b3:51:71:2f:6a:ee:4c:05:09:77:d3:
    72:62:a4:9b:d7
ASN1 OID: prime256v1
NIST CURVE: P-256
#+END_SRC

*** 验证签名
回顾签名流程:
1. 生成ASN.1 DER格式的tbsCertificate
2. 使用摘要算法，计算tbsCertificate摘要值
3. 签发者(issuer)使用自己的私钥，使用signatureAlgorithm对摘要进行签名，得到signatureValue

对应地，我们的验证流程:
1. 提取提取ASN.1 DER格式的tbsCertificate
2. 使用摘要算法，计算tbsCertificate摘要值 hash1
3. 提取证书的SignatureValue
4. 使用签发者(issuer)公钥，证书的摘要值hash1，证书的signatureValue，进行RSA签名认证。

我们可以使用`openssl` 命令, 将｀2｀ `3` `4` 合成一步：
#+BEGIN_SRC 
openssl sha256 <yuantops.tbs -verify Cloudflare_Inc_ECC_CA-3.pub -signature cloudflaressl.sig
Verified OK
#+END_SRC

或者，将`3` `4` 合成一步：
#+BEGIN_SRC 
openssl sha256 <yuantops.tbs -binary >hash
#+END_SRC

#+BEGIN_SRC 
openssl pkeyutl -verify -in hash -sigfile cloudflaressl.sig -inkey Cloudflare_Inc_ECC_CA-3.pub -pubin -pkeyopt digest:sha256
Signature Verified Successfully
#+END_SRC

到此，证书签名验证结束。

*** 其他　 
**** 为什么x509证书中，signatureValue要进行bit string编码?
https://crypto.stackexchange.com/questions/55574/why-is-the-signature-field-in-x-509-a-bit-string-despite-there-being-asn-1-der

https://security.stackexchange.com/questions/161982/asn-1-encapsulated-bitstring-type-in-openssl
**** 为什么不解密SignatureValue，将得到的hash值与tbsCertificate的hash值比较?
这一点上，我还没有特别明白。

[[http://yongbingchen.github.io/blog/2015/04/09/verify-the-signature-of-a-x-dot-509-certificate/][这篇文章]]　完全没有用到openssl验证签名，他手动用公钥解出了signature对应的hash值。

但是，我Google公钥解密签名的方法，回答都说不能 *解密*，只能 *验证*. 

知乎问题　[[https://www.zhihu.com/question/25912483][RSA的公钥和私钥到底哪个才是用来加密和哪个用来解密？]] 下面 刘巍然的回答 详细论述了RSA加解密算法和签名体制的区别，他说道:

#+BEGIN_QUOTE
在签名算法中，私钥用于对数据进行签名，公钥用于对签名进行验证。这也可以直观地进行理解：对一个文件签名，当然要用私钥，因为我们希望只有自己才能完成签字。验证过程当然希望所有人都能够执行，大家看到签名都能通过验证证明确实是我自己签的。
#+END_QUOTE

看来，似乎确实不能根据公钥对签名进行 *解密*?
* Financial Technology :FinTech:
** 理论扫盲-清算 
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: tech
:EXPORT_FILE_NAME: financial_fundamentals_clearing
:EXPORT_DESCRIPTION: 关于"清算"的学习笔记，主要摘抄总结自《支付清算理论与实务》
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "金融术语" )
:END:

先放豆瓣链接: [[https://book.douban.com/subject/27088012/][支付清算理论与实务]] 。 起因是最近接触到交易领域一些术语，尤其对“清算”有一些似是而非的理解。把一些有助自己理解的部分摘录在此。

*** 支付过程 P8
#+BEGIN_SRC txt
目前，支付过程的划分主要遵循国际清算银行支付结算委员会的划分方法，将支付处理过程划分为三个标准化过程，即交易过程（transaction），清算过程（clearing）和结算过程（settlement）。

交易过程：包括支付的产生、确认和发送，特别是对交易有关各方的身份确认、对支付工具的确认以及对支付能力的确认。

清算过程：包含了在收付款人金融机构之间交换支付工具以及计算金融机构之间待结算的债权，支付工具的交换也包括交易撮合、交易清分、数据收集等。

结算过程：该过程是完成债权最终转移的过程，包括收集待结算的债权并进性完整性检验、保证结算资金具有可用性、结清金融机构之间的债权债务以及记录和通知各方。
#+END_SRC

#+BEGIN_SRC txt
一般而言，结算过程完成后，该支付交易就最终完成了。结算中使用的资产也成为最终结算资产，最终结算资产可以是中央银行货币，也可以是商业银行货币。为了保证支付的安全性，保证交易各方的利益，结算过程一旦完成后，支付交易一般是不可撤销的，因此，结算过程的完成通常标志着该项或该次支付交易交易全过程的结束，标志着对应的商品交易债权债务的最终清偿。……因此，支付过程是一个完整的过程，或者说，支付应具有完整性。

#+END_SRC
*** 清算与结算
    
#+BEGIN_SRC txt
……
在这一定义中，清算(clearing)是指结算前对支付工具清分、撮合，对待结算的债权债务进性计算、轧差的过程，例如，在支票支付过程中的票据的收集、清分、轧差；在银行卡的支付过程中，对通过POS机形成的支付指令的传递，对银行卡信息的路由和传递；对日切前当日各个会员银行间待结算债权债务的轧差，计算出各个参与会员的应收应付额等，这些活动都属于清算的范畴。结算(settlement)主要是指各个结算银行间资金的最终转移的过程，包括收集待结算的债权并进性完整性检验、保证结算资金具有可用性，结清金融机构之间的债权债务以及记录和通知各方。这些任务中，最为重要的是对相应账户的处理，通过对账户的借记和贷记处理，资金从一个账户转移到另一个账户。因此，从这个意义上可以说，清算是为了更高效率完成结算的经济行为，结算是对清算后的债权债务最终的资金转移行为。为了进一步提高效率，众多银行机构的支付工具的清算过程都集中于一家机构进性处理，即所谓的集中清算，如银联集中处理所有银行的银行卡清算，而同城票据清算所处理该区域所有会员的票据。其实，区别纯清算机构有个简单的方法，纯清算机构不持有其成员机构的账户，而客户的结算账户是结算银行或结算机构的必要条件之一。

#+END_SRC

#+BEGIN_SRC 
我国目前所使用的“清算”与“结算”就是与 clearing 和 settlement 相对应的。但不是一一对应的：不论是“清算”还是“结算”，其含义都包含clearing和settlement这两个过程。
#+END_SRC

#+BEGIN_SRC txt
产生这些“模糊”认识的原因，可能与如下的事实有关：按照我国金融界传统习惯上的认识，一般将中央银行为其他商业银行提供的支付相关的结算服务成为“清算”，而将商业银行为企事业单位和个人客户提供的支付服务成为“银行结算”，或者简称“结算”。
#+END_SRC
*** 清算模式 P41
#+BEGIN_SRC txt
在现代支付系统中，轧差的方式是区分不同系统的主要依据，理论上，我们将轧差的方式分为实时全额、双边净额和多边净额方式等，实时清算模式下的结算没有时延，因此其最终的结算方式称为实时结算，而由于双边净额和多边净额不可避免的产生时延，因此其最终的结算方式称为延时结算。
#+END_SRC   

**** 实时全额
#+BEGIN_SRC txt
     实时全额清算模式是一种没有延时的清算模式，其特点是实时发送、逐笔处理、全额清算资金，建立在实时全额清算模式上的资金结算系统成为 RTGS 系统（Real Time Gross Settlement system）。由于 RTGS 要求支付方拥有足够的头寸，并且具有实时到账的特性，因此，RTGS 系统减少了清算资金的信用风险敞口，广泛为各国中央银行采用，例如美国的 Fedwire, 加拿大的 LVPS，英国的 CHAPS，欧盟的 TARGETS，中国的大额支付 HVPS 等系统都采取了 RTGS 模式。
     但是，由于实时全额清算模式意味着每一支付都需要足额的资金保证，因此，RTGS 系统需要参与者具有充足的支付头寸，对参与银行的流动性管理提出了较高的要求，因此，实时全额模式也被称为“资金饥渴”型模式。……
#+END_SRC   

#+BEGIN_SRC txt

#+END_SRC
* Java Guru
** artifact存在, 但maven报错: Could not resolve artifact
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: maven_cannot_resolve_local_artifact_error
:EXPORT_DESCRIPTION: 记录 maven 3.0.x 一个大坑
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "maven" )
:END:
如果你遇到这个问题，而local repository里jar确实存在，一定看一眼你使用的maven版本：你可能遇到maven 3的一个坑。

简而言之，maven3 开始验证本地仓库jar包的repository_id。

*** 原因
从maven3开始，从远程仓库下载jar包时，会在jar文件旁边生成一个`_maven.repositories`文件，文件里写明它来自哪个repository。 

如果当前项目的effective pom(`mvn help:effective-pom` 查看)里，生效的repository列表不包含这个jar包的repository_id，就会 *报错* 。

*** 解决办法
简单粗暴: 把`_maven.repositories`全删掉
#+BEGIN_SRC sh
find ~/.m2/repository -name _maven.repositories -exec rm -v {} \;
#+END_SRC

*** 参考
参考 [[https://stackoverflow.com/questions/16866978/maven-cant-find-my-local-artifacts][StackOverflow网友回答]] 。

** Java 使用指定 classloader 创建 class
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: define_class_with_custom_classloader
:EXPORT_DESCRIPTION: 一个 Java hack
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "java" )
:END:

有时需要在程序运行时动态创建 Java 类（加载自定义文件，或者是加载 Javassist 之类字节码增强工具创建出来的字节码等）。要注意的是，不同类加载器加载的类，彼此是不可见的，也就不能直接实例化。

要突破这个限制，需要一点 hack: 利用反射机制，在根据字节码创建类时，指定 classloader。下面的代码从著名的 ~jodd~ 库摘录，请自行学习。

#+BEGIN_SRC java 
    /**
     * Defines a class from byte array into the specified class loader.
     * Warning: this is a <b>hack</b>!
     * @param className optional class name, may be <code>null</code>
     * @param classData bytecode data
     * @param classLoader classloader that will load class
     */
    public static Class defineClass(final String className, final byte[] classData, ClassLoader classLoader) {
        if (classLoader == null) {
            classLoader = Thread.currentThread().getContextClassLoader();
        }
        try {
            final Method defineClassMethod = ClassLoader.class.getDeclaredMethod("defineClass", String.class, byte[].class, int.class, int.class);
            defineClassMethod.setAccessible(true);
            return (Class) defineClassMethod.invoke(classLoader, className, classData, 0, classData.length);
        } catch (Throwable th) {
            throw new RuntimeException("Define class failed: " + className, th);
        }
    }
#+END_SRC

** 使用自定义 Classloader 加载类，利用反射创建实例时出现 NoSuchMethodException 
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: NoSuchMethodException_when_using_classloader_and_reflection
:EXPORT_DESCRIPTION: java 中的类是由类的全名以及类的 classloader 来限定的；同一个类被不同 classloader 加载，它们将变成不同的类
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "java" )
:END:

最近在做一个需求，需要在程序运行时, 从当前 classpath 之外的指定路径加载(已经编译好的)类，并创建它的实例对象。

在程序运行时改变程序结构，本是动态语言的技能点，不是 Java 的强项。但借助 Java 语言 ~JavaCompiler~ 与 ~反射~ 等动态相关特性，也能勉强做到。 好在就这个需求而言，我们拿到的是编译好的 ~.class~ ，不需要编译开始从头做起。 

所以我们就从类的动态加载出发，开始做了。 下面是实施步骤，以及遇到的问题。

*** 步骤
1. 我们使用了 Github 上一个可以动态加载 Maven 类的依赖库: [[https://github.com/nanosai/modrun][ModRun]]。
2. 将要加载的类所在 jar 包，连同它依赖的 jar 包，按 maven repository 目录结构放置。得到 ModRun 的一个 moduleClassloader。
3. 用 moduleClassLoader 加载类，得到 Class ~clazz~ 。
4. 使用反射，调用 ~clazz.getDeclaredConstructor(xxxType1.class, xxxType2.class)~ ，得到构建函数。
5. 构建函数调用 ~invoke()~, 传入参数，预期得到所需要的对象。

*** 问题
进行到第 4 步，会报错，提示没有对应的构造函数。但肉眼看上去，同样签名的构造函数明明存在。何故？

*** 分析
在 StackOverflow 搜到答案: [[https://stackoverflow.com/questions/2999824/classcastexception-when-creating-an-instance-of-a-class-using-reflection-and-cla][StackOverflow 网友回答]] 。网友回复道: Since class objects depend on the actual class *AND* on the classloader, they are really different classes。

用 IDEA 断点调试，观察报错点，查看第 4 步入参的 classloader，与 clazz 的 classloader 确实不一样。如果改为传入 moduleClassLoader 加载的类，报错会消失，走到第 5 步；第 5 步会报错: object is not an instance of declaring class

原因不变，还是因为传入的对象，与调用者不属于同一个 classloader，虽然类名相同，也是不同类。

*** 解决方法
放弃使用 ModRun ，用自定义的 ClassLoader 替代。在实现这个 ClassLoader 时，要将当前使用的 ClassLoader 设置为 parent。依据双亲委托机制，这样满足可见性原则。可以参考 [[https://github.com/eclipse-vertx/vert.x/blob/master/src/main/java/io/vertx/core/impl/IsolatingClassLoader.java][IsolatingClassLoader.java]] 。

*** Java 类加载机制三大原则
1. 委托原则： 如果一个类还没有被加载，类加载器会委托它的父加载器去加载它。
2. 可见性原则: 被父亲类加载器加载的类对于孩子加载器是可见的，但关系相反相反则不可见。
3. 独特性原则: 当一个类加载器加载一个类时，它的孩子加载器绝不会重新加载这个类。

*** 参考资料 
1. Java 类加载器（ ClassLoader）浅析: https://blog.csdn.net/BIAOBIAOqi/article/details/6864567
2. Class Loaders in Java: https://www.baeldung.com/java-classloaders

** 分布式追踪系统之我见
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: thoughts_on_distributed_tracing_system
:EXPORT_DESCRIPTION: 什么是分布式追踪系统？使用它的 tradeoff 是什么？简单写写我的看法。   
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "java" "tracing" )
:END:

**** 一切的开始
古早的时候，没人需要分布式追踪系统。大家系统架构简单，功能直来直去，有问题就看日志、查问题。    

后来，微服务流行起来。之前的模式，被称作“单体架构”。微服务粒度更小了，以前一个接口做的事，现在要拆散成很多部分，再通过相互调用组合到一起。系统更瘦了，但是管理起来更难了。     

当我有 10 个微服务时，出了故障还可以排查。如果有成千上万个微服务呢？随着时间发展，微服务之间的依赖会越来越复杂，一个接口背后可能是几十上百个微服务调用，没人能搞懂了。

大家意识到，需要一个能展示整条调用链路情况的辅助系统。

**** Google Dapper 
这时候，Google 公开了一篇论文 [[https://research.google.com/archive/papers/dapper-2010-1.pdf][Dapper - a Large-Scale Distributed Systems Tracing Infrastructure]]，介绍他们的分布式追踪技术。这篇文章提出了科学的分布式追踪模型，仿佛一盏指路明灯，此后几乎所有实现，都遵循这个模型。    

基于这篇论文，有了 Zipkin(Twitter 开源), 有了 OpenTracing。国内阿里鹰眼，也在概念上有借鉴。

**** 分布式追踪系统
一般而言，分布式追踪系统分为三部分，采集、上报、落盘与分析。

- 采集: 对于已经存在大量系统，在代码中进行改造，工作量将相当可观，不现实。明智做法，是从中间件着手，力求侵入更小、开发者无感知。如果是 dubbo 调用，最合适是统一升级 dubbo jar 包。

- 上报：先把日志打印到本地，然后公共 agent 采集上报。

- 落盘与存储：数据采集上来后，可以做很多事情。最简单的，放到 HBase + ElasticSearch，支持按 traceId 搜索。复杂点，接入流计算引擎，实时计算相关指标。

**** 我真的需要它吗
直说我的看法：

- 如果你的系统调用链深度顶多三层，依赖外部系统才一两个，常规日志监控手段足矣，分布式追踪系统并不是必须品。
 
- 如果当前你的系统已经按微服务组织起来，但没有使用统一维护的中间件，那应该先改造现有系统，把中间件收敛起来，再统一升级。

**** 不错的参考资料 

1. 阿里巴巴鹰眼技术解密 https://www.cnblogs.com/gzxbkk/p/9600263.html

2. 分布式跟踪系统（一）：Zipkin的背景和设计 https://blog.csdn.net/manzhizhen/article/details/52811600

** 偶遇 static 初始化死锁
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: deadlock_in_static_initialization
:EXPORT_DESCRIPTION: 死锁有很多，在类初始化流程遭遇死锁比较少见。这里提供一份示例代码，恰好复现这种死锁。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "java" "deadlock")
:END:

最近开发新系统，用到了内存数据库 H2 web . 上线时，遇到问题：服务启动流程卡住，不报错，也起不来。

用 `jstack` 查看栈信息，main thread 状态为 RUNNABLE，另外一个线程 `H2 Console Server` 状态也为 RUNNABLE。仔细观察， ~main~ 栈包含一段可疑信息: ~-locked <xxxx> (a java.lang.Class for org.h2.Driver)~ ，疑似线程被锁。 

有两个诡异之处：1. 服务启动阻塞具有偶然性，有的机器（特别是性能比较差的机器）大概率不会卡。2. 虽然阻塞了，但此时 h2 数据库 web console 已经成功起来了。WHY??

作为应急措施，在启动 H2 web 之后，立马让当前线程停一会儿(~Thread.sleep(500)~)，服务顺利启动。接下来，是我慢慢和它死磕的过程。

*** 调试
单步调试法 + 日志大法。

调试小技巧：如果类在依赖的 jar 里，又想在这个类里打日志，只需按它的包结构在 IDE 里对应建立目录层级，再把源码拷贝进去就好了。

最后发现了问题：在类静态方法加载过程中，形成了死锁。

下面是摘出来的问题代码:

*** 问题代码

#+BEGIN_SRC java -n
import org.h2.Driver;
import org.h2.tools.Server;

import java.sql.SQLException;

public class Main {
    public static void main(String[] args1) throws InterruptedException {

        int port = 9081;
        Server webServer = null;
        String[] args = ("-web,-webAllowOthers,-trace,-webPort," + port).split(",");
        try {
            webServer = org.h2.tools.Server.createWebServer(args);
            webServer.start();
        } catch (SQLException e) {
            String msg = "h2 web server create failed";
            throw new IllegalStateException(msg, e);
        }

        System.out.println("["+Thread.currentThread().toString()+"]尝试加载 driver " + System.currentTimeMillis());
        Thread.sleep(32);
        Driver driver = new Driver();

        System.out.println("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!看到我，说明死锁重现失败。多试几次！！！！！！！！！！！！");
    }
}
#+END_SRC
*** 死锁分析
1. main() 方法运行的线程，为 [main] 线程。
2. 执行到第 14 行，执行 webServer.start() 方法后，第二个线程 [H2 Console Thread] 将被启动(通过单步追踪，可以定位具体代码)。具体流程为:
   2.1. 创建 ServerSocket, 开始监听服务端端口
   2.2. 新建名为[H2 Console Thread] 的线程，每当服务端口读到了数据，则
   2.3. 新分配一个线程，处理数据流。

3. 在 2.1 ，为了确认 ServerSocket 成功建立，会尝试与服务端口号建立一个 socket，一旦成功，则认为服务端口已经起来, 并关闭 socket。
   
4. 2.3 中创建的新线程，读到一个空白文件流，抛出 EOF 异常。在异常处理类 ~DbException.java~ 中，会加载 DriverManager.java (详见代码)。进入 static {} 后，持有了锁B。同时在 static 方法中，它通过 ServiceLoader 尝试加载所有 Driver，这意味着，它要等待 Driver.java 初始化的锁(锁A)

5. main 线程中，在故意等待了一小段时间(第21行)后，main() 方法继续执行。在第 22 行创建 driver 实例的过程中，进入static {} 方法，持有了 Driver.java 的锁(锁A)。同时，在 static {} 中，尝试 load DriverManager.java，需要等待 DriverManager.java 的锁(锁B)

6. 两个线程，彼此等待对方持有的锁。也就形成了死锁。

      
*** 自问自答: 初始化存在锁??
    参考回答: https://stackoverflow.com/questions/878577/are-java-static-initializers-thread-safe
    
    静态代码块 static {} 是线程安全的，同时只能在一个线程中运行。
    
    以及回答：https://stackoverflow.com/questions/55204559/what-happens-when-multiple-threads-ask-for-the-same-class-to-be-loaded-at-same-t
    
    类的初始化，确实存在锁。

*** 自问自答: 为什么 ~Thread.sleep(500)~ 有用?
    死锁的本质在资源争抢。加上 Java 类加载存在缓存机制，只要让一个线程先执行完，就破解了锁。
    
*** 他山之玉
另外，搜到一篇类似博文，都是由 ~DriverManager~ 在多个线程被初始化形成锁: [[https://medium.com/@priyaaggarwal24/avoiding-deadlock-when-using-multiple-jdbc-drivers-in-an-application-ce0b9464ecdf][Avoiding deadlock when using Multiple JDBC Drivers in an Application]]。
* Miscellaneous
** 关于连接池大小
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: about_pool_sizing
:EXPORT_DESCRIPTION: 连接池设多大合适，是个好问题；但肯定不是越大越好
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "pool" )
:END:

这篇文章讲得很好，值得一读:
*** About Pool Sizing
https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing

*** 结论
综合CPU核数，磁盘IO，网络状况，得到一个经验公式:
#+BEGIN_QUOTE
connections = ((core_count * 2) + effective_spindle_count)
#+END_QUOTE

#+BEGIN_SRC txt
A formula which has held up pretty well across a lot of benchmarks for years is
that for optimal throughput the number of active connections should be somewhere
near ((core_count * 2) + effective_spindle_count). Core count should not include
HT threads, even if hyperthreading is enabled. Effective spindle count is zero if
the active data set is fully cached, and approaches the actual number of spindles
as the cache hit rate falls. ... There hasn't been any analysis so far regarding
how well the formula works with SSDs.
#+END_SRC
