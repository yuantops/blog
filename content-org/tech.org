#+author: yuan.tops@gmail.com
#+hugo_base_dir: ../
#+HUGO_SECTION: tech
# Categories
#+filetags: @Tech
#+hugo_auto_set_lastmod: t

* All about this blog                                                           :gh_pages:hugo:
** DONE How to blog with ox-hugo in Emacs                                       :Emacs:
   CLOSED: [2019-07-25 Thu 14:47]
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_DATE: "2019-07-24T22:29:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2019-07-24T22:29:52Z"
:EXPORT_FILE_NAME: blogging-with-ox-hugo
:EXPORT_DESCRIPTION: 发现一个用Emacs写blog的新工具: ox-hugo。它与org-mode融合得非常自然，更好用。本文记录如何在Emacs中配置与使用ox-hugo。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "ox-hugo" )
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2019-07-25 Thu 14:47]
- State "TODO"       from "DONE"       [2019-07-25 Thu 10:05]
:END:

我的[[https://blog.yuantops.com/tech/emacs-orgmode-hugo-with-oxpandoc/][一篇老博客]]，介绍了用Emacs和Org-mode写博客的工作流。总的来说，勉强符合预期，流程稍显磕绊。今天偶然发现，用Org-mode写博客有了正规军： ~ox-hugo~ 。简单试用之后,发现 ~ox-hugo~ 表现很流畅，几乎就是对之前流程的升级，于是毫不犹豫选 ~ox-hugo~ 。

*** 安装
~ox-hugo~ 是一个工作在Org-mode的Emacs包，安装过程可以说平平无奇：
1. ~M x~ ~package-refresh-contents~ :: 更新安装源
2. 添加Emacs配置。修改细节与Emacs配置风格相关，我使用[[https://github.com/purcell/emacs.d][Purcell维护的.emacs.d配置]]，仅供参考。
   - 在 =.emacs.d/lisp/= 目录下，新增配置文件 ~init-ox-hugo.el~
     #+BEGIN_SRC emacs-lisp -n
     (require-package 'ox-hugo)

     (with-eval-after-load 'ox (require 'ox-hugo))

     (provide 'init-ox-hugo)

     #+END_SRC

   - 在入口配置文件 ~.emacs.d/init.el~ 中，引用新增的配置文件
     #+BEGIN_SRC lisp

     (require 'init-ox-hugo)

     #+END_SRC

*** 写博客旧流程
现在，org文件可以直接导出为Hugo支持的Markdown格式了。这一点 ~ox-pandoc~ 也能做到，但 ~ox-hugo~ 还能做得更多。

回顾之前的工作流:
1. 新建xx.org文件
2. 输入二级标题，接着插入 ~front matter~ 。 ~front matter~ 内容大致如下：
   #+BEGIN_SRC yaml
   +++
   title = ""
   date = ""
   Categories = ["Tech"]
   Tags = ["Emacs"]
   Description = ""
   keywords = [""]
   +++
   #+END_SRC
3. 写博客正文
4. 按下 ~C-c C-e~ , 将二级标题对应的subtree导出为Markdown格式文件
5. 将Markdown格式文件保存到contents/tech/目录。

~front matter~ 是hugo渲染文件需要的必要元信息, 例如文章标签、分类、标题等。ox-hugo改进之一，是使用org-mode语法放置元信息，不再需要手动设置 ~front matter~ 。

Org-mode支持为headline设置 tags。 ~ox-hugo~ 沿用这点，org-mode的tag就是文章标签, 以@开头的tag就是文章分类。其他例如作者、日期、文件名的元信息，通过 ~:PROPERTIES:~ 设置。在导出Markdown格式时，ox-hugo会自动提取这些数据。因为ox-hugo可以一次导出所有subtree，因此 ~ox-hugo~ 官方推荐做法是，所有文章放到一个org文件，[[https://ox-hugo.scripter.co/doc/screenshots/#screenshot-one-post-per-subtree][每个Subtree对应一篇文章]]。

对于习惯一个org文件对应一篇文章的作者而言，ox-hugo也予以尊重，[[https://ox-hugo.scripter.co/doc/org-meta-data-to-hugo-front-matter/][在文件头添加对应配置项即可]]。

*** 我的实践
对本博客而言，考虑到1)文章固定只有3个分类，2)现在已有很多存量md格式的文章，我采取一种折中做法: 一个分类对应一个org文件，org文件里每个subtree对应一篇文章。这样兼顾现状，而且新文章顺利迁移到新做法。就每个org文件来说，元信息分为两大类：
1. 对整个文件都生效的配置，放到文件开头；
   #+BEGIN_SRC yaml
   #+author: yuan.tops@gmail.com
   #+hugo_base_dir: ../
   #+HUGO_SECTION: tech
   # Categories
   #+filetags: @tech
   #+hugo_auto_set_lastmod: t
   #+END_SRC
2. 对单篇文章生效的配置，放到subtree的标题下面。
   #+BEGIN_SRC props
   :PROPERTIES:
   :EXPORT_HUGO_CATEGORIES: Tech
   :EXPORT_DATE: "2019-07-24T22:29:52+08:00"
   :EXPORT_HUGO_PUBLISHDATE: "2019-07-24T22:29:52Z"
   :EXPORT_FILE_NAME: blogging-with-ox-hugo
   :EXPORT_DESCRIPTION: 发现一个用Emacs写blog的新工具: ox-hugo。它与org-mode融合得非常自然，更好用。本文记录如何在Emacs中配置与使用ox-hugo。
   :END:
   #+END_SRC

以本文为例，配置如图:
[[file:screenshot-org-subtree.png]]

*** 总结
Emacs学习曲线盘旋上升, Org-mode也是如此。不断折腾，乐在其中！


** DONE 浏览器会处理URL里的相对路径
   CLOSED: [2019-11-21 Thu 15:53]
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: how_web_broswer_handles_url_relative_path
:EXPORT_DESCRIPTION: 如果要访问的url包含相对路径，浏览器会尝试解析相对路径，再访问解析得到的地址。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "http" )
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2019-11-21 Thu 15:53]
:END:

新系统上线前，安全部门扫描出一个高危漏洞：文件任意下载漏洞。渗透测试人员在URL里加上相对路径，不断发起HTTP请求，居然成功下载到Linux系统密码文件。修复漏洞挺简单，限制HTTP服务访问文件系统权限，不允许超出指定目录，几行代码搞定。

修复之后准备进行验证，第一步当然是复现漏洞。没想到，这一步就挺曲折。

*** 消失的点号
打开chrome，在地址栏输入带了相对路径( *..* )的URL。URL指向一个文件，理论上，会触发文件下载。结果是：地址栏URL路径里点号全不见了，变成了一个正常地址。多试几遍，把双点号换成单点号，仍然如此。用 wiresharks 抓包看HTTP报文，请求头 /path/ 没有点号。这说明，浏览器做了手脚。

1. Google搜之，找到一份解析URI的RFC3968标准，专门有一章论述解析点号：[[https://tools.ietf.org/html/rfc3986#section-5.2.4][Remove Dot Segments]]。经过解析，点号和双点号会消失，这个过程被称为 *remove_dot_segments* 。(RFC3968给出了这个过程的伪代码。)

2. Google官方在一篇文章里，将Chrome解析URL的过程称为 `Canonicalization` ([[https://chromium.googlesource.com/chromium/src/+/master/docs/security/url_display_guidelines/url_display_guidelines.md#display-urls-in-canonical-form][display-urls-in-canonical-form]]) 。经过解析，Chrome地址栏的点号变成实际值。

结合两篇文档，原理清楚了：浏览器遵循RFC3968规范处理URL相对路径，所以点号和双点号都被干掉了。

*** 改用Burp Suite重现问题
不能用浏览器复现问题，改尝试 *curl* 命令。结果，curl也不能复现。好在可以借助 *Burp Suite* 工具。

Burp Suite是一款攻击web服务的集成工具，一般黑客用它来渗透网络。我们牛刀小用，用来拦截、修改HTTP请求报文。过程不在此赘述。总之，用它绕过了相对路径解析、重现了漏洞。

*** 修复漏洞
略。

* Raspeberry Tutorials                                                          :树莓派:
** TODO 用树莓派和Calibre搭建电子书服务器

* Road to linux expert                                                          :Linux:
** Bash Guideline Notes
:PROPERTIES:
:EXPORT_DATE: "2019-07-25T22:29:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2019-07-25T22:29:52Z"
:EXPORT_FILE_NAME: bash-guideline-study-notes
:EXPORT_DESCRIPTION: 《Bash Guideline》摘抄与笔记
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "bash" )
:END:
*** 关于重定向顺序
#+BEGIN_QUOTE

Note that the order of redirections is signi cant. For example, the command \\
\\
ls > dirlist 2>&1 \\
directs both standard output ( file descriptor 1) and standard error ( le descriptor 2) to the file dirlist, while the command ls 2>&1 > dirlist directs only the standard output to file dirlist, because the standard error was made a copy of the standard output before the standard output was redirected to dirlist. \\
#+END_QUOTE

*** 将Stdout 和 Stderr 重定向到 文件
#+BEGIN_QUOTE
This construct allows both the standard output ( file descriptor 1) and the standard error output ( file descriptor 2) to be redirected to the file whose name is the expansion of word. \\
\\
There are two formats for redirecting standard output and standard error:\\
\\
&>word and \\
\\
>&word
\\
Of the two forms, the first is preferred. This is semantically equivalent to\\
>word 2>&1\\
#+END_QUOTE

*** Here Document

#+BEGIN_SRC
Here Documents
This type of redirection instructs the shell to read input from the current source until a line containing only word (with no trailing blanks) is seen.

All of the lines read up to that point are then used as the standard input for a command.

The format of here-documents is:

          <<[-]word
                  here-document
          delimiter
No parameter expansion, command substitution, arithmetic expansion, or pathname expansion is performed on word. If any characters in word are quoted, the delimiter is the result of quote removal on word, and the lines in the here-document are not expanded. If word is unquoted, all lines of the here-document are subjected to parameter expansion, command substitution, and arithmetic expansion. In the latter case, the character sequence \<newline> is ignored, and \ must be used to quote the characters \, $, and `.

If the redirection operator is <<-, then all leading tab characters are stripped from input lines and the line containing delimiter. This allows here-documents within shell scripts to be indented in a natural fashion.

$ cat <<EOF > print.sh
#!/bin/bash
echo \$PWD
echo $PWD
EOF
#+END_SRC

** Understanding XOR
:PROPERTIES:
:EXPORT_DATE: "2019-07-25T15:29:52+08:00"
:EXPORT_HUGO_PUBLISHDATE: "2019-07-25T22:29:52Z"
:EXPORT_FILE_NAME: understanding-xor
:EXPORT_DESCRIPTION: 理解xor
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "xor" )
:END:

#+BEGIN_QUOTE
We can interpret the action of XOR in a number of different ways, and this helps to shed light on its properties. The most obvious way to interpret it is as its name suggests, ‘exclusive OR’: A ⊕ B is true if and only if precisely one of A and B is true. Another way to think of it is as identifying difference in a pair of bytes: A ⊕ B = ‘the bits where they differ’. This interpretation makes it obvious that A ⊕ A = 0 (byte A does not differ from itself in any bit) and A ⊕ 0 = A (byte A differs from 0 precisely in the bit positions that equal 1) and is also useful when thinking about toggling and encryption later on. \\
\\
The last, and most powerful, interpretation of XOR is in terms of parity, i.e. whether something is odd or even. For any n bits, A1 ⊕ A2 ⊕ … ⊕ An = 1 if and only if the number of 1s is odd. This can be proved quite easily by induction and use of associativity. It is the crucial observation that leads to many of the properties that follow, including error detection, data protection and adding. \\
\\
 Essentially the combined value x ^ y ‘remembers’ both states, and one state is the key to getting at the other.
#+END_QUOTE

* Golang is great                                                               :Golang:
* Cryptography
** 浏览器验证SSL数字证书的步骤
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: how_do_web_broswer_validate_ssl_certificates
:EXPORT_DESCRIPTION: 本文介绍通过https协议访问网站时，浏览器验证服务器SSL证书的原理，并以Chrome访问百度为例进行分析。
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "ssl" )
:END:

浏览器和服务器使用SSL/TLS通信时，双方首先要通过几次握手(Handshake)，建立加密信道。简单说来，分为下面３步:
1. 服务器发送自己的SSL证书；
2. 浏览器验证服务器SSL证书；
3. 证书验证成功，双方协商得到对称加密密钥，并交换。双方拿到对称加密密钥后，后续的通信都会用它做对称加密。

本文介绍的重点，在前２步。首先，转载一篇国外博客，讲述浏览器检查证书的过程；其次，会引述两个RFC协议的相关内容；最后，wireshark抓包进行验证。

*** Browsers and Certificate Validation
原文地址: https://www.ssl.com/article/browsers-and-certificate-validation/

使用[[https://www.deepl.com/en/translator][DeepL]] 翻译成中文，如下:

#+BEGIN_SRC 
## 证书和X.509格式
证书在各方面都是数字文件，这意味着它们需要遵循一种文件格式来存储信息（如签名、密钥、签发人等）。虽然私有的PKI配置可以为其证书实现任何格式，但公共信任的PKIs（即那些被浏览器信任的PKIs）必须符合RFC 5280，这就要求使用X.509 v3格式。

X.509 v3允许证书包含额外的数据，如使用限制或策略信息，作为扩展，每个扩展都是关键或非关键的。浏览器可以忽略无效的或未被识别的非关键扩展，但它们必须处理和验证所有关键扩展。

## 认证路径和路径处理
憑證機構使用私人密碼匙對所有簽發的證書進行加密簽署。这种签名可以不可撤销地证明证书是由某一特定的核证机 构签发的，而且在签署后没有被修改。

CA通过持有相应公钥的自发证书（称为根）来建立其签名密钥的所有权。憑證機構必須遵守嚴格的控制和審核程序來建立、管理和使用根證書，為了減少暴露，通常會使用根證書來簽發中間證書。这些中间证书可以用来签发客户的证书。
浏览器在出厂时都有一个内置的可信根列表。(这些根是来自通过浏览器严格标准的CA的根。) 为了验证证书，浏览器将获得一个证书序列，每个证书都签署了序列中的下一个证书，将签名CA的根与服务器的证书连接起来。

这个证书序列称为认证路径。路径的根部称为信任锚，服务器的证书称为叶子或终端实体证书。

### 路径的构造
通常情况下，浏览器必须考虑多个认证路径，直到他们能够为给定证书找到一个有效的路径。即使一个路径可能包含的证书可以正确地 "链 "到一个已知的锚，但由于路径长度、域名、证书使用或政策的限制，路径本身可能会被拒绝。

对于浏览器遇到的每一个新证书，构建和评估所有可能的路径都是一个昂贵的过程。浏览器已经实现了各种优化，以减少被拒绝的候选路径的数量，但深入探讨这些细节已经超出了本文的范围。

### 路径验证
候选认证路径构建完成后，浏览器使用证书中包含的信息对其进行验证。如果浏览器能够通过密码学的方式证明，从一个信任锚直接签署的证书开始，每个证书对应的私钥都被用来签发路径中的下一个证书，一直到叶子证书，那么这个路径就是有效的。

## 认证路径验证算法
RFC 5280描述了浏览器验证X.509证书认证路径的标准算法。

基本上，浏览器从信任锚（即根证书）开始，遍历路径中的所有证书，验证每张证书的基本信息和关键扩展。

如果该过程以路径中的最后一张证书结束，没有错误，那么该路径被接受为有效。如果产生错误，则该路径被标记为无效。

### 证书的基本处理
无论是否有任何扩展，浏览器必须始终验证基本的证书信息，如签名或签发人。下面的章节显示了浏览器执行检查的顺序。

1. 浏览器验证证书的完整性
证书上的签名可以用正常的公用钥匙加密法进行验证。如果签名无效，则认为该证书在签发后被修改，因此被拒绝。

2. 浏览器验证证书的有效性：
證書的有效期是指簽署憑證機構保證會維持其狀態資訊的時間間隔。浏览器会拒绝任何有效期在验证检查日期和时间之前或之后开始的证书。

3. 浏览器检查证书的撤销状态。
证书签发后，应该在整个有效期内使用。当然，在各种情况下，证书可能会在自然到期前失效。

这类情况可能包括主体改名或怀疑私钥泄露。在这样的情况下，CA需要撤销相应的证书，而用户也信任CA会通知浏览器其证书的撤销状态。

RFC 5280建议CA使用撤销列表来实现这一目的。

证书废止列表(CRL)
核證機關會定期發出一份經簽署、有時間標記的廢止證書清單，稱為證書廢止清單（CRL）。CRL分布在公开的存储库中，浏览器在验证证书时可以获得并查阅CA的最新CRL。

这种方法的一个缺陷是，撤销的时间粒度仅限于CRL的发布期。只有在所有当前已发布的CRL都被安排更新后，浏览器才会收到撤销的通知。根据签名CA的政策，这可能需要一个小时、一天甚至一周的时间。

在线证书状态协议(OCSP)
还有其他的方法来获取废止状态信息，其中最流行的是在线证书状态协议（OCSP）。

OCSP在标准文档RFC6960中进行了描述，它允许浏览器从在线OCSP服务器（也称为回复者）请求特定证书的撤销状态。如果配置得当，OCSP的即时性更强，而且避免了上面提到的CRL更新延迟问题。此外，OCSP Stapling还能提高性能和速度。

4. 浏览器验证发件人
证书通常与两个实体相关联。

签发人，也就是拥有签名密钥的实体，以及
主体，指的是证书认证的公钥的所有者。
浏览器会检查证书的签发人字段是否与路径中前一个证书的主题字段相同。为了增加安全性，大多数PKI实现也会验证发证者的密钥是否与签署当前证书的密钥相同。(请注意，这对于信任锚来说并不正确，因为根是自发的--即它们具有相同的签发人和主体)。

约束处理
X.509 v3格式允许CA定义约束或限制每个证书如何被验证和作为关键扩展使用。路径中的每张证书都可以施加额外的约束，所有后续证书都必须遵守。

证书约束很少影响普通互联网用户，尽管它们在企业SSL解决方案中相当常见。功能性约束可以达到多种操作目的，但其最重要的用途是缓解已知的安全问题。

5. 浏览器检查名称约束
具有适当名称限制的私有(但公众信任的)中间CA可以为组织提供对证书管理和签发的精细控制。证书可以被限制在一个公司或组织的域名的特定域或域树（即包括子域）。名称限制通常用于从公开信任的CA购买的中间CA证书，以防止中间CA为第三方域（如google.com）签发完全有效的证书。

6. 浏览器检查策略约束
證書政策是由核證機關所發表的法律文件，正式詳述其簽發及管理證書的程序。憑證機構可以根據一項或多項政策簽發證書，而每張證書都有這些政策的連結，以便信賴者在決定信任該證書前，可以評估這些政策。

出于法律和操作上的原因，证书可以对证书的政策进行限制。如果发现证书中包含关键策略约束，浏览器必须在进行之前对其进行验证。(然而，关键策略约束在现实世界中很少遇到，所以本文其余部分将不予考虑)。

7. 浏览器检查基本约束（也就是路径长度）。
X.509 v3格式允许签发人定义证书所能支持的最大路径长度。这提供了对每个证书在认证路径中可以放置多远的控制。这实际上是很重要的--浏览器曾经无视认证路径长度，直到一位研究人员在2009年的一次演讲中演示了他如何利用自己网站的叶子证书为一个大型电子商务网站伪造有效证书。

8. 浏览器验证密钥用途
钥匙用途 "扩展部分说明了证书中钥匙的用途，例如加密、签名、证书签名等。这些用途的例子包括加密、签名、证书签名等。浏览器会拒绝违反其密钥用途限制的证书，例如遇到服务器证书的密钥只用于CRL签名。

9. 浏览器继续处理所有剩余的关键扩展文件
浏览器在处理完上述扩展证书后，会继续验证当前证书指定为关键的所有剩余扩展证书，然后再进入下一个。如果浏览器到达一个路径的叶子证书时没有错误，那么该路径就会被接受为有效。如果产生任何错误，则路径被标记为无效，并且不能建立安全连接。

通过www.DeepL.com/Translator（免费版）翻译
 #+END_SRC

*** 两个重要RFC标准
上面转载的文章，详细讲述了浏览器的验证过程。细心人会发现，文章多次提到RFC 5280。这个标准定义了X.509证书格式，是互联网加密体系中处于基石地位的标准之一。除了RFC 5280，还有一个比较重要的标准 RFC 5246，定义了SSL传输层协议。

**** [RFC 5280] PKI X.509 v3规范　
https://tools.ietf.org/html/rfc5280   

***** 路径验证算法
其中，section-6 给出了证书validation算法，并给出了路径验证算法（section-6）。
在此，摘录关键部分: 
#+BEGIN_SRC  text
 (a)  for all x in {1, ..., n-1}, the subject of certificate x is
           the issuer of certificate x+1;

 (b)  certificate 1 is issued by the trust anchor;

 (c)  certificate n is the certificate to be validated (i.e., the
      target certificate); and

 (d)  for all x in {1, ..., n}, the certificate was valid at the
           time in question
#+END_SRC 

根据算法，第一个证书由trust anchor签发，下一个证书由这个证书签发……直到最后的叶子节点证书。这样由信任锚长出一个链条，一环扣一环，链条上每个节点都是可信的。

***** 证书指纹(fingerprint)验证算法
x509如何计算fingerprint?
https://stackoverflow.com/questions/4803799/how-to-calculate-x-509-certificates-sha-1-fingerprint

生成过程: 1) 根据signature算法，计算出证书TBS部分的signature 2) 证书签发者issuer用自己的私钥加密signature，得到fingerprint
验证过程: 1) 根据signature算法，计算出证书TBS部分的signature 2) 使用证书签发者issuer的公钥解密fingerprint, 得到signature 3) 比对两个signature

**** [RFC 5246] TLS 1.2规范
https://tools.ietf.org/html/rfc5246   

其中，section-7.4.2　规定，server要向client发送 /certificate_list/ 。服务器不是返回单独的某个证书，而是一个证书列表; 因为单独一个证书，没法形成certifate chain，也就无法完成validation: 这和[RFC 5280] 所述流程吻合。 

#+BEGIN_SRC text  
certificate_list
      This is a sequence (chain) of certificates.  The sender's
      certificate MUST come first in the list.  Each following
      certificate MUST directly certify the one preceding it.  Because
      certificate validation requires that root keys be distributed
      independently, the self-signed certificate that specifies the root
      certificate authority MAY be omitted from the chain, under the
      assumption that the remote end must already possess it in order to
      validate it in any case.
#+END_SRC 

这里指定了证书的顺序，第一个是叶子证书。很好理解，因为重要数据在报文中的位置往往靠前。

*** 抓包看看　
借助wireshark，我们实际操作一番。  

打开wireshark, 开始抓包，再访问百度官网(https://www.baidu.com)。抓包细节如下：　

[[file:e:/blog/static/ox-hugo/ssl_certificate_wireshark.png][wireshark抓包分析]]


** 手工验证一张数字证书的有效性
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: validate_a_digital_certificate_step_by_step
:EXPORT_DESCRIPTION: 尽可能细致地实践证书验证算法
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "ssl" "CA" )
:END:
上一篇 [[https://blog.yuantops.com/tech/how_do_web_broswer_validate_ssl_certificates/][博客]] 讨论浏览器验证数字证书的流程。这篇文章更深入一步，用原始方法一步步手工验证证书的合法性。本文主要参考: [[https://security.stackexchange.com/questions/127095/manually-walking-through-the-signature-validation-of-a-certificate][回答]] 与 [[https://segmentfault.com/a/1190000019008423][X.509、PKCS文件格式介绍]]。

*** 基础名词
**** ASN.1, DER与PEM
ASN.1是一种接口描述语言，它用来定义一种数据结构。

DER是一种编码规则，它用二进制表示ASN.1定义的数据。很多密码学标准使用ASN.1定义数据结构，用DER编码。

但因为DER的内容是二进制的，不方便传输，人们对DER二进制内容进行Base64编码，将其转换为ASCII码，并在头和尾加上标签，就是PEM格式。PEM全称Privacy-Enhanced Mail，起初是为了便于邮件传输，后来在很多场景得到广泛应用。

**** X.509
X.509是RFC5280定义的一种公钥证书格式(public key certificate)。X.509证书也被称为数字Digital Certificate。一张X.509包含一个Public Key和一个身份信息。X.509证书要么是自签发，要么是被CA签发。

*** 如何得到一张证书
借助浏览器，可以方便导出数字证书。

打开chrome，访问本博客网址(https://blog.yuantops.com)，地址栏最左侧有个小锁图案 —— 这是网站受到HTTPS加密保护的标志。

在"Details"标签，观察"Certificate Subject Alternative Name"字段，值包含"DNS Name: yuantops.com" "DNS　Name: *.yuantops.com"，说明证书的确属于这个域名。

点击小锁　-> "certificate" -> "Details" -> "Export..."，可以选择证书的导出格式。

选择"Base64-encoded ASCII, single certificate"，得到一张PEM格式证书。将它保存为`sni.cloudflaressl.com`。
#+BEGIN_SRC txt
-----BEGIN CERTIFICATE-----
MIIEwzCCBGmgAwIBAgIQDVZy4W9/IjNEOZEGQ2ADTjAKBggqhkjOPQQDAjBKMQsw
CQYDVQQGEwJVUzEZMBcGA1UEChMQQ2xvdWRmbGFyZSwgSW5jLjEgMB4GA1UEAxMX
Q2xvdWRmbGFyZSBJbmMgRUNDIENBLTMwHhcNMjAwODA3MDAwMDAwWhcNMjEwODA3
MTIwMDAwWjBtMQswCQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNh
biBGcmFuY2lzY28xGTAXBgNVBAoTEENsb3VkZmxhcmUsIEluYy4xHjAcBgNVBAMT
FXNuaS5jbG91ZGZsYXJlc3NsLmNvbTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IA
BCh3/Sz4YWHFP32cBLzErjTKy4/AdFKU37wFK8kzP7sdhM3/BxdJNKeRYNwcDimw
k76zgHaaGki0AzvCTMa+llWjggMMMIIDCDAfBgNVHSMEGDAWgBSlzjfq67B1DpRn
iLRF+tkkEIeWHzAdBgNVHQ4EFgQUi9pqgIAX5apgTXwOGZ9k1FALDL0wPgYDVR0R
BDcwNYIOKi55dWFudG9wcy5jb22CFXNuaS5jbG91ZGZsYXJlc3NsLmNvbYIMeXVh
bnRvcHMuY29tMA4GA1UdDwEB/wQEAwIHgDAdBgNVHSUEFjAUBggrBgEFBQcDAQYI
KwYBBQUHAwIwewYDVR0fBHQwcjA3oDWgM4YxaHR0cDovL2NybDMuZGlnaWNlcnQu
Y29tL0Nsb3VkZmxhcmVJbmNFQ0NDQS0zLmNybDA3oDWgM4YxaHR0cDovL2NybDQu
ZGlnaWNlcnQuY29tL0Nsb3VkZmxhcmVJbmNFQ0NDQS0zLmNybDBMBgNVHSAERTBD
MDcGCWCGSAGG/WwBATAqMCgGCCsGAQUFBwIBFhxodHRwczovL3d3dy5kaWdpY2Vy
dC5jb20vQ1BTMAgGBmeBDAECAjB2BggrBgEFBQcBAQRqMGgwJAYIKwYBBQUHMAGG
GGh0dHA6Ly9vY3NwLmRpZ2ljZXJ0LmNvbTBABggrBgEFBQcwAoY0aHR0cDovL2Nh
Y2VydHMuZGlnaWNlcnQuY29tL0Nsb3VkZmxhcmVJbmNFQ0NDQS0zLmNydDAMBgNV
HRMBAf8EAjAAMIIBBAYKKwYBBAHWeQIEAgSB9QSB8gDwAHYA9lyUL9F3MCIUVBgI
MJRWjuNNExkzv98MLyALzE7xZOMAAAFzyS9NoAAABAMARzBFAiB5au5KCRfkyBcI
7jECy/NvNPkKEoMUUTwZP+rZbHtn8AIhAKOR2Lh2zsCw+gy38abKie1fyd1rmm0c
GA/pP6PykChvAHYAXNxDkv7mq0VEsV6a1FbmEDf71fpH3KFzlLJe5vbHDsoAAAFz
yS9N0wAABAMARzBFAiALkQMvm51FKVO2JRFiWWEgqu4x9rGHy2JH6P2m18lrLQIh
AN1PcRtCiY+gihkncncx18OZM6e5CGZruk05EDGThLTvMAoGCCqGSM49BAMCA0gA
MEUCIHXeLOwERMHY88NliKhUzs1MwoJap9sNm9qQLGXYCpEMAiEA1ZsGvWxusXK9
tAgwUjlWi5Ke5rvM/i01sYl6bpls4Z0=
-----END CERTIFICATE-----
#+END_SRC

*** 分析证书结构
RFC5280规定了X.509证书的语法:
#+BEGIN_SRC txt
   Certificate  ::=  SEQUENCE  {
        tbsCertificate       TBSCertificate,
        signatureAlgorithm   AlgorithmIdentifier,
        signatureValue       BIT STRING  }
#+END_SRC

根据定义,证书由TBSCertificate, 签名算法，签名值三部分构成。 我们可以将它们分别提取出来。提取之前，先观察证书结构:

#+BEGIN_SRC shell
openssl asn1parse -i -in sni.cloudflaressl.com 
#+END_SRC
***** 选项解释
`-in filename`: 输入文件名

`-i`: 标记实体，输出缩进标记，将一个ASN1实体下的其他对象缩进显示。此选项非默认选项，加上此选项后，显示更易看懂。
***** 输出
#+BEGIN_SRC txt
    0:d=0  hl=4 l=1219 cons: SEQUENCE          
    4:d=1  hl=4 l=1129 cons:  SEQUENCE          
    8:d=2  hl=2 l=   3 cons:   cont [ 0 ]        
   10:d=3  hl=2 l=   1 prim:    INTEGER           :02
   13:d=2  hl=2 l=  16 prim:   INTEGER           :0D5672E16F7F2233443991064360034E
   31:d=2  hl=2 l=  10 cons:   SEQUENCE          
   33:d=3  hl=2 l=   8 prim:    OBJECT            :ecdsa-with-SHA256
   43:d=2  hl=2 l=  74 cons:   SEQUENCE          
   45:d=3  hl=2 l=  11 cons:    SET               
   47:d=4  hl=2 l=   9 cons:     SEQUENCE          
   49:d=5  hl=2 l=   3 prim:      OBJECT            :countryName
   54:d=5  hl=2 l=   2 prim:      PRINTABLESTRING   :US
   58:d=3  hl=2 l=  25 cons:    SET               
   60:d=4  hl=2 l=  23 cons:     SEQUENCE          
   62:d=5  hl=2 l=   3 prim:      OBJECT            :organizationName
   67:d=5  hl=2 l=  16 prim:      PRINTABLESTRING   :Cloudflare, Inc.
   85:d=3  hl=2 l=  32 cons:    SET               
   87:d=4  hl=2 l=  30 cons:     SEQUENCE          
   89:d=5  hl=2 l=   3 prim:      OBJECT            :commonName
   94:d=5  hl=2 l=  23 prim:      PRINTABLESTRING   :Cloudflare Inc ECC CA-3
  119:d=2  hl=2 l=  30 cons:   SEQUENCE          
  121:d=3  hl=2 l=  13 prim:    UTCTIME           :200807000000Z
  136:d=3  hl=2 l=  13 prim:    UTCTIME           :210807120000Z
  151:d=2  hl=2 l= 109 cons:   SEQUENCE          
  153:d=3  hl=2 l=  11 cons:    SET               
  155:d=4  hl=2 l=   9 cons:     SEQUENCE          
  157:d=5  hl=2 l=   3 prim:      OBJECT            :countryName
  162:d=5  hl=2 l=   2 prim:      PRINTABLESTRING   :US
  166:d=3  hl=2 l=  11 cons:    SET               
  168:d=4  hl=2 l=   9 cons:     SEQUENCE          
  170:d=5  hl=2 l=   3 prim:      OBJECT            :stateOrProvinceName
  175:d=5  hl=2 l=   2 prim:      PRINTABLESTRING   :CA
  179:d=3  hl=2 l=  22 cons:    SET               
  181:d=4  hl=2 l=  20 cons:     SEQUENCE          
  183:d=5  hl=2 l=   3 prim:      OBJECT            :localityName
  188:d=5  hl=2 l=  13 prim:      PRINTABLESTRING   :San Francisco
  203:d=3  hl=2 l=  25 cons:    SET               
  205:d=4  hl=2 l=  23 cons:     SEQUENCE          
  207:d=5  hl=2 l=   3 prim:      OBJECT            :organizationName
  212:d=5  hl=2 l=  16 prim:      PRINTABLESTRING   :Cloudflare, Inc.
  230:d=3  hl=2 l=  30 cons:    SET               
  232:d=4  hl=2 l=  28 cons:     SEQUENCE          
  234:d=5  hl=2 l=   3 prim:      OBJECT            :commonName
  239:d=5  hl=2 l=  21 prim:      PRINTABLESTRING   :sni.cloudflaressl.com
  262:d=2  hl=2 l=  89 cons:   SEQUENCE          
  264:d=3  hl=2 l=  19 cons:    SEQUENCE          
  266:d=4  hl=2 l=   7 prim:     OBJECT            :id-ecPublicKey
  275:d=4  hl=2 l=   8 prim:     OBJECT            :prime256v1
  285:d=3  hl=2 l=  66 prim:    BIT STRING        
  353:d=2  hl=4 l= 780 cons:   cont [ 3 ]        
  357:d=3  hl=4 l= 776 cons:    SEQUENCE          
  361:d=4  hl=2 l=  31 cons:     SEQUENCE          
  363:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Authority Key Identifier
  368:d=5  hl=2 l=  24 prim:      OCTET STRING      [HEX DUMP]:30168014A5CE37EAEBB0750E946788B445FAD9241087961F
  394:d=4  hl=2 l=  29 cons:     SEQUENCE          
  396:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Subject Key Identifier
  401:d=5  hl=2 l=  22 prim:      OCTET STRING      [HEX DUMP]:04148BDA6A808017E5AA604D7C0E199F64D4500B0CBD
  425:d=4  hl=2 l=  62 cons:     SEQUENCE          
  427:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Subject Alternative Name
  432:d=5  hl=2 l=  55 prim:      OCTET STRING      [HEX DUMP]:3035820E2A2E7975616E746F70732E636F6D8215736E692E636C6F7564666C61726573736C2E636F6D820C7975616E746F70732E636F6D
  489:d=4  hl=2 l=  14 cons:     SEQUENCE          
  491:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Key Usage
  496:d=5  hl=2 l=   1 prim:      BOOLEAN           :255
  499:d=5  hl=2 l=   4 prim:      OCTET STRING      [HEX DUMP]:03020780
  505:d=4  hl=2 l=  29 cons:     SEQUENCE          
  507:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Extended Key Usage
  512:d=5  hl=2 l=  22 prim:      OCTET STRING      [HEX DUMP]:301406082B0601050507030106082B06010505070302
  536:d=4  hl=2 l= 123 cons:     SEQUENCE          
  538:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 CRL Distribution Points
  543:d=5  hl=2 l= 116 prim:      OCTET STRING      [HEX DUMP]:30723037A035A0338631687474703A2F2F63726C332E64696769636572742E636F6D2F436C6F7564666C617265496E6345434343412D332E63726C3037A035A0338631687474703A2F2F63726C342E64696769636572742E636F6D2F436C6F7564666C617265496E6345434343412D332E63726C
  661:d=4  hl=2 l=  76 cons:     SEQUENCE          
  663:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Certificate Policies
  668:d=5  hl=2 l=  69 prim:      OCTET STRING      [HEX DUMP]:3043303706096086480186FD6C0101302A302806082B06010505070201161C68747470733A2F2F7777772E64696769636572742E636F6D2F4350533008060667810C010202
  739:d=4  hl=2 l= 118 cons:     SEQUENCE          
  741:d=5  hl=2 l=   8 prim:      OBJECT            :Authority Information Access
  751:d=5  hl=2 l= 106 prim:      OCTET STRING      [HEX DUMP]:3068302406082B060105050730018618687474703A2F2F6F6373702E64696769636572742E636F6D304006082B060105050730028634687474703A2F2F636163657274732E64696769636572742E636F6D2F436C6F7564666C617265496E6345434343412D332E637274
  859:d=4  hl=2 l=  12 cons:     SEQUENCE          
  861:d=5  hl=2 l=   3 prim:      OBJECT            :X509v3 Basic Constraints
  866:d=5  hl=2 l=   1 prim:      BOOLEAN           :255
  869:d=5  hl=2 l=   2 prim:      OCTET STRING      [HEX DUMP]:3000
  873:d=4  hl=4 l= 260 cons:     SEQUENCE          
  877:d=5  hl=2 l=  10 prim:      OBJECT            :CT Precertificate SCTs
  889:d=5  hl=3 l= 245 prim:      OCTET STRING      [HEX DUMP]:0481F200F0007600F65C942FD1773022145418083094568EE34D131933BFDF0C2F200BCC4EF164E300000173C92F4DA000000403004730450220796AEE4A0917E4C81708EE3102CBF36F34F90A128314513C193FEAD96C7B67F0022100A391D8B876CEC0B0FA0CB7F1A6CA89ED5FC9DD6B9A6D1C180FE93FA3F290286F0076005CDC4392FEE6AB4544B15E9AD456E61037FBD5FA47DCA17394B25EE6F6C70ECA00000173C92F4DD3000004030047304502200B91032F9B9D452953B6251162596120AAEE31F6B187CB6247E8FDA6D7C96B2D022100DD4F711B42898FA08A1927727731D7C39933A7B908666BBA4D3910319384B4EF
 1137:d=1  hl=2 l=  10 cons:  SEQUENCE          
 1139:d=2  hl=2 l=   8 prim:   OBJECT            :ecdsa-with-SHA256
 1149:d=1  hl=2 l=  72 prim:  BIT STRING        
 #+END_SRC

***** 输出格式解析
#+BEGIN_SRC 
    0:d=0  hl=4 l=1219 cons: SEQUENCE          
 #+END_SRC

`0`: 表示节点在整个文件中的偏移长度
`d=0`: 表示节点深度
`hl=4`: 表示节点头字节长度
`l=1219`: 表示节点数据字节长度
`cons`: 表示该节点为结构节点，表示包含子节点或者子结构数据
`prim`: 表示该节点为原始节点，包含数据

***** tbsCertificate和signature位置
观察可知，tbsCertificate的偏移位置是4, 签名值signatureValue的偏移位置是1137。

*** 提取tbsCertificate
引用RFC5280 4.1.1.3:
#+BEGIN_SRC txt
   The signatureValue field contains a digital signature computed upon
   the ASN.1 DER encoded tbsCertificate.  The ASN.1 DER encoded
   tbsCertificate is used as the input to the signature function. 
#+END_SRC

根据上述定义，计算签名的输入是DER编码的tbsCertificate。而我们从浏览器导出的证书是PEM格式，需要使用openssl将其转化为DER格式。
#+BEGIN_SRC 
openssl x509 -in sni.cloudflaressl.com -inform PEM -out sni.cloudflaressl.com.der -outform DER
#+END_SRC

然后，我们从DER证书中提取tbsCertificate。 根据asn1parse输出结果第二行，tbsCertificate偏移位置是4, 大小是1133 = ４(头部长度) + 1129(数据长度)。
#+BEGIN_SRC 
    4:d=1  hl=4 l=1129 cons:  SEQUENCE          
#+END_SRC

使用`dd` 按偏移位置截取。输出内容保存到`yuantops.tbs`。
#+BEGIN_SRC 
dd if=sni.cloudflaressl.com.der of=yuantops.tbs skip=4 bs=1 count=1133
#+END_SRC

*** 提取signatureValue
根据asn1parse输出结果末尾一行:

#+BEGIN_SRC 
 1149:d=1  hl=2 l=  72 prim:  BIT STRING        
#+END_SRC

signatureValue偏移量是1137。如果直接使用`dd`截取，将得到`BIT STRING`编码后的签名值，不能直接使用。需要用`ans1parse`的`-strparse`选项，将其转换为二进制数据。

#+BEGIN_SRC shell
 openssl asn1parse -in sni.cloudflaressl.com -strparse 1137 -out cloudflaressl.sig
#+END_SRC

签名数据保存在cloudflaressl.sig文件。

***** 选项解释
`-in filename` ：输入文件名，默认为标准输入。

`-offset number`：开始数据分析的字节偏移量，分析数据时，不一定从头开始分析，可用指定偏移量，默认从头开始分析。

`-strparse offset`：此选项也用于从一个偏移量开始来分析数据，不过，与-offset不一样。-offset分析偏移量之后的所有数据，而-strparse只用于分析一段数据，并且这种数据必须是SET或者SEQUENCE，它只分析本SET或者SEQUENCE范围的数据。

***** 查看提取的签名是否正确

使用`od`命令，以16进制打印签名文件内容: 
#+BEGIN_SRC 
`od -tx1 cloudflaressl.sig`
0000000 30 45 02 20 75 de 2c ec 04 44 c1 d8 f3 c3 65 88
0000020 a8 54 ce cd 4c c2 82 5a a7 db 0d 9b da 90 2c 65
0000040 d8 0a 91 0c 02 21 00 d5 9b 06 bd 6c 6e b1 72 bd
0000060 b4 08 30 52 39 56 8b 92 9e e6 bb cc fe 2d 35 b1
0000100 89 7a 6e 99 6c e1 9d
0000107
#+END_SRC
与浏览器Certificate Viewer中看到的证书`SignatureValue`对比，二者应该相同。

下一步，我们获取签发者的公钥。
*** 获取issuer公钥
在上一篇博客中提到，服务器返回给浏览器一组证书链。通过浏览器Certificate Viewer可以看到证书继承关系。 `sni.cloudflaressl.com`证书的签发者是`Cloudflare Inc ECC CA-3`。

我们将其导出为文件，保存到本地，文件名为 `Cloudflare_Inc_ECC_CA-3`。

从证书提取公钥:
#+BEGIN_SRC 
openssl x509 -in Cloudflare_Inc_ECC_CA-3 -noout -pubkey > Cloudflare_Inc_ECC_CA-3.pub
#+END_SRC

如果想观察公钥内容，可以将其转换为PEM格式:
#+BEGIN_SRC text
openssl pkey -in Cloudflare_Inc_ECC_CA-3.pub -pubin -text

-----BEGIN PUBLIC KEY-----
MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEua1NZpkUC0bsH4HRKlAenQMVLzQS
fS2WuIg4m4Vfj7+7Te9hRsTJc9QkT+DuHM5ss1FxL2ruTAUJd9NyYqSb1w==
-----END PUBLIC KEY-----
Public-Key: (256 bit)
pub: 
    04:b9:ad:4d:66:99:14:0b:46:ec:1f:81:d1:2a:50:
    1e:9d:03:15:2f:34:12:7d:2d:96:b8:88:38:9b:85:
    5f:8f:bf:bb:4d:ef:61:46:c4:c9:73:d4:24:4f:e0:
    ee:1c:ce:6c:b3:51:71:2f:6a:ee:4c:05:09:77:d3:
    72:62:a4:9b:d7
ASN1 OID: prime256v1
NIST CURVE: P-256
#+END_SRC

*** 验证签名
回顾签名流程:
1. 生成ASN.1 DER格式的tbsCertificate
2. 使用摘要算法，计算tbsCertificate摘要值
3. 签发者(issuer)使用自己的私钥，使用signatureAlgorithm对摘要进行签名，得到signatureValue

对应地，我们的验证流程:
1. 提取提取ASN.1 DER格式的tbsCertificate
2. 使用摘要算法，计算tbsCertificate摘要值 hash1
3. 提取证书的SignatureValue
4. 使用签发者(issuer)公钥，证书的摘要值hash1，证书的signatureValue，进行RSA签名认证。

我们可以使用`openssl` 命令, 将｀2｀ `3` `4` 合成一步：
#+BEGIN_SRC 
openssl sha256 <yuantops.tbs -verify Cloudflare_Inc_ECC_CA-3.pub -signature cloudflaressl.sig
Verified OK
#+END_SRC

或者，将`3` `4` 合成一步：
#+BEGIN_SRC 
openssl sha256 <yuantops.tbs -binary >hash
#+END_SRC

#+BEGIN_SRC 
openssl pkeyutl -verify -in hash -sigfile cloudflaressl.sig -inkey Cloudflare_Inc_ECC_CA-3.pub -pubin -pkeyopt digest:sha256
Signature Verified Successfully
#+END_SRC

到此，证书签名验证结束。

*** 其他　 
**** 为什么x509证书中，signatureValue要进行bit string编码?
https://crypto.stackexchange.com/questions/55574/why-is-the-signature-field-in-x-509-a-bit-string-despite-there-being-asn-1-der

https://security.stackexchange.com/questions/161982/asn-1-encapsulated-bitstring-type-in-openssl
**** 为什么不解密SignatureValue，将得到的hash值与tbsCertificate的hash值比较?
这一点上，我还没有特别明白。

[[http://yongbingchen.github.io/blog/2015/04/09/verify-the-signature-of-a-x-dot-509-certificate/][这篇文章]]　完全没有用到openssl验证签名，他手动用公钥解出了signature对应的hash值。

但是，我Google公钥解密签名的方法，回答都说不能 *解密*，只能 *验证*. 

知乎问题　[[https://www.zhihu.com/question/25912483][RSA的公钥和私钥到底哪个才是用来加密和哪个用来解密？]] 下面 刘巍然的回答 详细论述了RSA加解密算法和签名体制的区别，他说道:

#+BEGIN_QUOTE
在签名算法中，私钥用于对数据进行签名，公钥用于对签名进行验证。这也可以直观地进行理解：对一个文件签名，当然要用私钥，因为我们希望只有自己才能完成签字。验证过程当然希望所有人都能够执行，大家看到签名都能通过验证证明确实是我自己签的。
#+END_QUOTE

看来，似乎确实不能根据公钥对签名进行 *解密*?
* Java Guru
** artifact存在, 但maven报错: Could not resolve artifact
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: maven_cannot_resolve_local_artifact_error
:EXPORT_DESCRIPTION: 记录 maven 3.0.x 一个大坑
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "maven" )
:END:
如果你遇到这个问题，而local repository里jar确实存在，一定看一眼你使用的maven版本：你可能遇到maven 3的一个坑。

简而言之，maven3 开始验证本地仓库jar包的repository_id。

*** 原因
从maven3开始，从远程仓库下载jar包时，会在jar文件旁边生成一个`_maven.repositories`文件，文件里写明它来自哪个repository。 

如果当前项目的effective pom(`mvn help:effective-pom` 查看)里，生效的repository列表不包含这个jar包的repository_id，就会 *报错* 。

*** 解决办法
简单粗暴: 把`_maven.repositories`全删掉
#+BEGIN_SRC sh
find ~/.m2/repository -name _maven.repositories -exec rm -v {} \;
#+END_SRC

*** 参考
参考 [[https://stackoverflow.com/questions/16866978/maven-cant-find-my-local-artifacts][StackOverflow网友回答]] 。

** Java 使用指定 classloader 创建 class
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: define_class_with_custom_classloader
:EXPORT_DESCRIPTION: 一个 Java hack
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "java" )
:END:

有时需要在程序运行时动态创建 Java 类（加载自定义文件，或者是加载 Javassist 之类字节码增强工具创建出来的字节码等）。要注意的是，不同类加载器加载的类，彼此是不可见的，也就不能直接实例化。

要突破这个限制，需要一点 hack: 利用反射机制，在根据字节码创建类时，指定 classloader。下面的代码从著名的 ~jodd~ 库摘录，请自行学习。

#+BEGIN_SRC java 
    /**
     * Defines a class from byte array into the specified class loader.
     * Warning: this is a <b>hack</b>!
     * @param className optional class name, may be <code>null</code>
     * @param classData bytecode data
     * @param classLoader classloader that will load class
     */
    public static Class defineClass(final String className, final byte[] classData, ClassLoader classLoader) {
        if (classLoader == null) {
            classLoader = Thread.currentThread().getContextClassLoader();
        }
        try {
            final Method defineClassMethod = ClassLoader.class.getDeclaredMethod("defineClass", String.class, byte[].class, int.class, int.class);
            defineClassMethod.setAccessible(true);
            return (Class) defineClassMethod.invoke(classLoader, className, classData, 0, classData.length);
        } catch (Throwable th) {
            throw new RuntimeException("Define class failed: " + className, th);
        }
    }
#+END_SRC

** 使用自定义 Classloader 加载类，利用反射创建实例时出现 NoSuchMethodException 
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: NoSuchMethodException_when_using_classloader_and_reflection
:EXPORT_DESCRIPTION: java 中的类是由类的全名以及类的 classloader 来限定的；同一个类被不同 classloader 加载，它们将变成不同的类
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "java" )
:END:

最近在做一个需求，需要在程序运行时, 从当前 classpath 之外的指定路径加载(已经编译好的)类，并创建它的实例对象。

在程序运行时改变程序结构，本是动态语言的技能点，不是 Java 的强项。但借助 Java 语言 ~JavaCompiler~ 与 ~反射~ 等动态相关特性，也能勉强做到。 好在就这个需求而言，我们拿到的是编译好的 ~.class~ ，不需要编译开始从头做起。 

所以我们就从类的动态加载出发，开始做了。 下面是实施步骤，以及遇到的问题。

*** 步骤
1. 我们使用了 Github 上一个可以动态加载 Maven 类的依赖库: [[https://github.com/nanosai/modrun][ModRun]]。
2. 将要加载的类所在 jar 包，连同它依赖的 jar 包，按 maven repository 目录结构放置。得到 ModRun 的一个 moduleClassloader。
3. 用 moduleClassLoader 加载类，得到 Class ~clazz~ 。
4. 使用反射，调用 ~clazz.getDeclaredConstructor(xxxType1.class, xxxType2.class)~ ，得到构建函数。
5. 构建函数调用 ~invoke()~, 传入参数，预期得到所需要的对象。

*** 问题
进行到第 4 步，会报错，提示没有对应的构造函数。但肉眼看上去，同样签名的构造函数明明存在。何故？

*** 分析
在 StackOverflow 搜到答案: [[https://stackoverflow.com/questions/2999824/classcastexception-when-creating-an-instance-of-a-class-using-reflection-and-cla][StackOverflow 网友回答]] 。网友回复道: Since class objects depend on the actual class *AND* on the classloader, they are really different classes。

用 IDEA 断点调试，观察报错点，查看第 4 步入参的 classloader，与 clazz 的 classloader 确实不一样。如果改为传入 moduleClassLoader 加载的类，报错会消失，走到第 5 步；第 5 步会报错: object is not an instance of declaring class

原因不变，还是因为传入的对象，与调用者不属于同一个 classloader，虽然类名相同，也是不同类。

*** 解决方法
放弃使用 ModRun ，用自定义的 ClassLoader 替代。在实现这个 ClassLoader 时，要将当前使用的 ClassLoader 设置为 parent。依据双亲委托机制，这样满足可见性原则。可以参考 [[https://github.com/eclipse-vertx/vert.x/blob/master/src/main/java/io/vertx/core/impl/IsolatingClassLoader.java][IsolatingClassLoader.java]] 。

*** Java 类加载机制三大原则
1. 委托原则： 如果一个类还没有被加载，类加载器会委托它的父加载器去加载它。
2. 可见性原则: 被父亲类加载器加载的类对于孩子加载器是可见的，但关系相反相反则不可见。
3. 独特性原则: 当一个类加载器加载一个类时，它的孩子加载器绝不会重新加载这个类。

*** 参考资料 
1. Java 类加载器（ ClassLoader）浅析: https://blog.csdn.net/BIAOBIAOqi/article/details/6864567
2. Class Loaders in Java: https://www.baeldung.com/java-classloaders

** 分布式追踪系统之我见
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: thoughts_on_distributed_tracing_system
:EXPORT_DESCRIPTION: 什么是分布式追踪系统？使用它的 tradeoff 是什么？简单写写我的看法。   
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "java" "tracing" )
:END:

**** 一切的开始
古早的时候，没人需要分布式追踪系统。大家系统架构简单，功能直来直去，有问题就看日志、查问题。    

后来，微服务流行起来。之前的模式，被称作“单体架构”。微服务粒度更小了，以前一个接口做的事，现在要拆散成很多部分，再通过相互调用组合到一起。系统更瘦了，但是管理起来更难了。     

当我有 10 个微服务时，出了故障还可以排查。如果有成千上万个微服务呢？随着时间发展，微服务之间的依赖会越来越复杂，一个接口背后可能是几十上百个微服务调用，没人能搞懂了。

大家意识到，需要一个能展示整条调用链路情况的辅助系统。

**** Google Dapper 
这时候，Google 公开了一篇论文 [[https://research.google.com/archive/papers/dapper-2010-1.pdf][Dapper - a Large-Scale Distributed Systems Tracing Infrastructure]]，介绍他们的分布式追踪技术。这篇文章提出了科学的分布式追踪模型，仿佛一盏指路明灯，此后几乎所有实现，都遵循这个模型。    

基于这篇论文，有了 Zipkin(Twitter 开源), 有了 OpenTracing。国内阿里鹰眼，也在概念上有借鉴。

**** 分布式追踪系统
一般而言，分布式追踪系统分为三部分，采集、上报、落盘与分析。

- 采集: 对于已经存在大量系统，在代码中进行改造，工作量将相当可观，不现实。明智做法，是从中间件着手，力求侵入更小、开发者无感知。如果是 dubbo 调用，最合适是统一升级 dubbo jar 包。

- 上报：先把日志打印到本地，然后公共 agent 采集上报。

- 落盘与存储：数据采集上来后，可以做很多事情。最简单的，放到 HBase + ElasticSearch，支持按 traceId 搜索。复杂点，接入流计算引擎，实时计算相关指标。

**** 我真的需要它吗
直说我的看法：

- 如果你的系统调用链深度顶多三层，依赖外部系统才一两个，常规日志监控手段足矣，分布式追踪系统并不是必须品。
 
- 如果当前你的系统已经按微服务组织起来，但没有使用统一维护的中间件，那应该先改造现有系统，把中间件收敛起来，再统一升级。

**** 不错的参考资料 

1. 阿里巴巴鹰眼技术解密 https://www.cnblogs.com/gzxbkk/p/9600263.html

2. 分布式跟踪系统（一）：Zipkin的背景和设计 https://blog.csdn.net/manzhizhen/article/details/52811600

* Miscellaneous
** 关于连接池大小
:PROPERTIES:
:EXPORT_HUGO_CATEGORIES: Tech
:EXPORT_FILE_NAME: about_pool_sizing
:EXPORT_DESCRIPTION: 连接池设多大合适，是个好问题；但肯定不是越大越好
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :keywords '( "pool" )
:END:

这篇文章讲得很好，值得一读:
*** About Pool Sizing
https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing

*** 结论
综合CPU核数，磁盘IO，网络状况，得到一个经验公式:
#+BEGIN_QUOTE
connections = ((core_count * 2) + effective_spindle_count)
#+END_QUOTE

#+BEGIN_SRC txt
A formula which has held up pretty well across a lot of benchmarks for years is
that for optimal throughput the number of active connections should be somewhere
near ((core_count * 2) + effective_spindle_count). Core count should not include
HT threads, even if hyperthreading is enabled. Effective spindle count is zero if
the active data set is fully cached, and approaches the actual number of spindles
as the cache hit rate falls. ... There hasn't been any analysis so far regarding
how well the formula works with SSDs.
#+END_SRC
